{
  "summary": {
    "total_projects": 30,
    "projects_with_github": 23,
    "github_percentage": 76.66666666666667,
    "average_quality_score": 0.7241666666666666,
    "average_categories_per_project": 5.4,
    "average_technologies_per_project": 1.9666666666666666,
    "unique_categories": 16,
    "unique_technologies": 15,
    "analysis_date": "2025-06-24T15:33:36.122348"
  },
  "category_distribution": {
    "Finance/Trading": 9,
    "Classification/Analysis": 8,
    "Evaluation/Benchmarking": 16,
    "Web Application": 22,
    "Multi-Agent Delegation": 2,
    "Research/Experimentation": 12,
    "Conversational AI": 9,
    "Multi-Agent Orchestration": 7,
    "Document Processing": 6,
    "API/Backend Service": 15,
    "Reinforcement Learning": 11,
    "Single Agent Application": 21,
    "Infrastructure/Tooling": 13,
    "Education/Learning": 7,
    "Data Processing/ETL": 3,
    "Healthcare/Medical": 1
  },
  "technology_distribution": {
    "PydanticAI": 4,
    "Logfire": 2,
    "OpenAI": 17,
    "Anthropic": 12,
    "JavaScript": 3,
    "Python": 7,
    "LangChain": 2,
    "Google AI": 3,
    "NumPy": 1,
    "LlamaIndex": 1,
    "React": 3,
    "AWS": 1,
    "Pandas": 1,
    "SQLite": 1,
    "Azure": 1
  },
  "timeline_data": [
    {
      "date": "2025-06-22T13:49:27.225793Z",
      "categories": [
        "Finance/Trading",
        "Classification/Analysis",
        "Evaluation/Benchmarking",
        "Web Application",
        "Multi-Agent Delegation",
        "Research/Experimentation",
        "Conversational AI",
        "Multi-Agent Orchestration"
      ],
      "technologies": [
        "PydanticAI",
        "Logfire",
        "OpenAI"
      ],
      "user": "Hendrik Reh"
    },
    {
      "date": "2025-06-22T18:26:51.546369Z",
      "categories": [
        "Document Processing",
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Reinforcement Learning",
        "Web Application",
        "Single Agent Application"
      ],
      "technologies": [
        "OpenAI",
        "Anthropic"
      ],
      "user": "Jeff Fedor"
    },
    {
      "date": "2025-06-22T19:13:21.168184Z",
      "categories": [
        "Infrastructure/Tooling",
        "Research/Experimentation",
        "API/Backend Service",
        "Classification/Analysis",
        "Reinforcement Learning",
        "Web Application",
        "Evaluation/Benchmarking",
        "Education/Learning"
      ],
      "technologies": [
        "JavaScript"
      ],
      "user": "Ty Bohan"
    },
    {
      "date": "2025-06-22T20:56:58.337825Z",
      "categories": [
        "Research/Experimentation",
        "Single Agent Application",
        "Infrastructure/Tooling",
        "Reinforcement Learning"
      ],
      "technologies": [
        "OpenAI"
      ],
      "user": "Paul Oreto"
    },
    {
      "date": "2025-06-22T22:36:36.168514Z",
      "categories": [
        "Web Application",
        "Single Agent Application"
      ],
      "technologies": [
        "OpenAI",
        "Anthropic"
      ],
      "user": "Ed G"
    },
    {
      "date": "2025-06-23T04:56:29.52156Z",
      "categories": [
        "Infrastructure/Tooling",
        "Evaluation/Benchmarking",
        "Reinforcement Learning",
        "Web Application",
        "Research/Experimentation",
        "Single Agent Application"
      ],
      "technologies": [
        "OpenAI",
        "Python"
      ],
      "user": "Tejas Khot"
    },
    {
      "date": "2025-06-23T05:07:39.855752Z",
      "categories": [
        "Finance/Trading",
        "Classification/Analysis",
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Web Application",
        "Data Processing/ETL",
        "Single Agent Application"
      ],
      "technologies": [
        "PydanticAI",
        "OpenAI"
      ],
      "user": "Kelly"
    },
    {
      "date": "2025-06-23T10:58:11.426114Z",
      "categories": [
        "Infrastructure/Tooling",
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Web Application",
        "Data Processing/ETL",
        "Conversational AI",
        "Single Agent Application"
      ],
      "technologies": [
        "JavaScript",
        "Python",
        "OpenAI",
        "Anthropic",
        "LangChain"
      ],
      "user": "Morteza"
    },
    {
      "date": "2025-06-23T13:32:14.368591Z",
      "categories": [
        "Finance/Trading",
        "Document Processing",
        "Infrastructure/Tooling",
        "API/Backend Service",
        "Classification/Analysis",
        "Evaluation/Benchmarking",
        "Web Application"
      ],
      "technologies": [
        "JavaScript",
        "Google AI",
        "NumPy",
        "LlamaIndex",
        "Anthropic"
      ],
      "user": "Isfandiyar Shaheen"
    },
    {
      "date": "2025-06-23T13:33:55.779891Z",
      "categories": [
        "Web Application",
        "Finance/Trading",
        "Research/Experimentation",
        "Single Agent Application"
      ],
      "technologies": [
        "Anthropic",
        "Python"
      ],
      "user": "Zaur Rzakhanov"
    },
    {
      "date": "2025-06-23T18:32:07.182124Z",
      "categories": [],
      "technologies": [],
      "user": "James T"
    },
    {
      "date": "2025-06-23T18:34:52.376418Z",
      "categories": [
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Web Application",
        "Education/Learning",
        "Conversational AI",
        "Single Agent Application"
      ],
      "technologies": [
        "React",
        "AWS"
      ],
      "user": "James T"
    },
    {
      "date": "2025-06-23T19:38:39.434316Z",
      "categories": [
        "Infrastructure/Tooling",
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Reinforcement Learning",
        "Web Application",
        "Single Agent Application"
      ],
      "technologies": [
        "Pandas",
        "Python"
      ],
      "user": "Louk de Loijer"
    },
    {
      "date": "2025-06-23T20:24:00.491076Z",
      "categories": [
        "Finance/Trading",
        "Research/Experimentation",
        "Web Application",
        "Education/Learning",
        "Conversational AI",
        "Single Agent Application"
      ],
      "technologies": [
        "Logfire",
        "OpenAI",
        "Anthropic",
        "Python"
      ],
      "user": "Johnny Heo"
    },
    {
      "date": "2025-06-23T20:56:17.090593Z",
      "categories": [
        "Web Application",
        "Research/Experimentation",
        "Single Agent Application",
        "Reinforcement Learning"
      ],
      "technologies": [
        "SQLite"
      ],
      "user": "Predrag Malicevic"
    },
    {
      "date": "2025-06-24T00:33:23.936069Z",
      "categories": [
        "Document Processing",
        "Infrastructure/Tooling",
        "API/Backend Service",
        "Reinforcement Learning",
        "Web Application",
        "Single Agent Application"
      ],
      "technologies": [
        "React",
        "LangChain",
        "OpenAI"
      ],
      "user": "Aswin Bhaskaran"
    },
    {
      "date": "2025-06-24T00:57:07.097434Z",
      "categories": [
        "Single Agent Application"
      ],
      "technologies": [
        "Google AI"
      ],
      "user": "Diana Padilla"
    },
    {
      "date": "2025-06-24T01:06:48.126875Z",
      "categories": [
        "Web Application",
        "Single Agent Application"
      ],
      "technologies": [],
      "user": "Skylar Payne"
    },
    {
      "date": "2025-06-24T02:59:38.083752Z",
      "categories": [
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Reinforcement Learning",
        "Conversational AI",
        "Single Agent Application"
      ],
      "technologies": [
        "OpenAI",
        "Anthropic"
      ],
      "user": "Zane Peycke"
    },
    {
      "date": "2025-06-24T03:05:21.050775Z",
      "categories": [
        "Finance/Trading",
        "Infrastructure/Tooling",
        "API/Backend Service",
        "Classification/Analysis",
        "Web Application",
        "Healthcare/Medical",
        "Research/Experimentation",
        "Multi-Agent Orchestration"
      ],
      "technologies": [
        "Google AI",
        "Anthropic"
      ],
      "user": "Jacob Milleville"
    },
    {
      "date": "2025-06-24T03:53:45.005985Z",
      "categories": [
        "Finance/Trading",
        "Infrastructure/Tooling",
        "Classification/Analysis",
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Web Application",
        "Research/Experimentation",
        "Multi-Agent Orchestration"
      ],
      "technologies": [
        "PydanticAI",
        "OpenAI",
        "Anthropic"
      ],
      "user": "Fernando Meira"
    },
    {
      "date": "2025-06-24T03:57:59.031891Z",
      "categories": [
        "Web Application",
        "Education/Learning",
        "Conversational AI",
        "Single Agent Application"
      ],
      "technologies": [
        "OpenAI"
      ],
      "user": "Brendan Rappazzo"
    },
    {
      "date": "2025-06-24T04:02:32.028461Z",
      "categories": [
        "Evaluation/Benchmarking",
        "Single Agent Application",
        "Reinforcement Learning"
      ],
      "technologies": [
        "OpenAI"
      ],
      "user": "Nick DeLuca"
    },
    {
      "date": "2025-06-24T05:30:39.215752Z",
      "categories": [
        "Evaluation/Benchmarking",
        "Single Agent Application",
        "Reinforcement Learning"
      ],
      "technologies": [],
      "user": "Trivan Menezes"
    },
    {
      "date": "2025-06-24T05:33:10.790818Z",
      "categories": [
        "Web Application",
        "Single Agent Application",
        "Infrastructure/Tooling"
      ],
      "technologies": [
        "Anthropic"
      ],
      "user": "Ari Pritchard-Bell"
    },
    {
      "date": "2025-06-24T08:42:13.242241Z",
      "categories": [
        "Education/Learning",
        "Conversational AI",
        "Document Processing",
        "Multi-Agent Orchestration"
      ],
      "technologies": [
        "Anthropic"
      ],
      "user": "James E Snewin"
    },
    {
      "date": "2025-06-24T10:23:54.285876Z",
      "categories": [
        "Document Processing",
        "Infrastructure/Tooling",
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Web Application",
        "Research/Experimentation",
        "Single Agent Application"
      ],
      "technologies": [
        "OpenAI"
      ],
      "user": "Hamza"
    },
    {
      "date": "2025-06-24T12:43:09.288056Z",
      "categories": [
        "Finance/Trading",
        "Document Processing",
        "Infrastructure/Tooling",
        "API/Backend Service",
        "Classification/Analysis",
        "Evaluation/Benchmarking",
        "Web Application",
        "Conversational AI",
        "Multi-Agent Orchestration"
      ],
      "technologies": [
        "Azure",
        "Python",
        "OpenAI",
        "Anthropic",
        "PydanticAI"
      ],
      "user": "Ilkka Lehto"
    },
    {
      "date": "2025-06-24T12:46:43.021419Z",
      "categories": [
        "Finance/Trading",
        "Infrastructure/Tooling",
        "Classification/Analysis",
        "Reinforcement Learning",
        "API/Backend Service",
        "Web Application",
        "Evaluation/Benchmarking",
        "Data Processing/ETL",
        "Multi-Agent Delegation",
        "Education/Learning",
        "Conversational AI",
        "Multi-Agent Orchestration"
      ],
      "technologies": [
        "OpenAI",
        "Python"
      ],
      "user": "Julian - Thomas Erdoedy"
    },
    {
      "date": "2025-06-24T15:02:22.517109Z",
      "categories": [
        "Research/Experimentation",
        "API/Backend Service",
        "Multi-Agent Orchestration"
      ],
      "technologies": [
        "React",
        "OpenAI"
      ],
      "user": "Kazuki Inamura"
    },
    {
      "date": "2025-06-24T16:33:47.046478Z",
      "categories": [
        "Research/Experimentation",
        "Education/Learning",
        "Single Agent Application"
      ],
      "technologies": [],
      "user": "Harry Jackson"
    }
  ],
  "projects": [
    {
      "user_name": "Hendrik Reh",
      "user_id": "cohort_20383_user_601555",
      "project_text": "**RFQ Multi-Agent System / RFQ Assistant**\n\n  \n\n[**Git-Repo**](https://github.com/HendrikReh/rfq-multi-agent-system)\n\n  \n\n**Project Overview**\n\nI built a **multi-agent system for intelligent RFQ (Request for Quote) processing** using the PydanticAI ecosystem. It features 4 specialized agents that collaboratively extract requirements, assess risk, perform competitive analysis, and generate proposals.\n\nThe system showcases advanced orchestration patterns, parallel execution, agent delegation, and Best-of-N selection with LLM-judge evaluation, combined with observability and evaluations.\n\n**What Approaches Did You Try?**\n\n**1\\. Multi-Agent Architecture Evolution**\n\n*   **Initial Approach:** Single RFQ parser agent for basic requirement extraction\n*   **Iterative Expansion:** Grew to 4 specialized agents through domain decomposition\n*   **Agent Specialization Strategy:** Each agent focused on a specific capability (parsing, competitive analysis, risk assessment, proposal writing)\n*   **PydanticAI Integration:** Leveraged structured outputs, tool usage, and agent delegation patterns\n*   **→ Started Simple, Scaled Systematically**\n\n**2\\. Orchestration Pattern Experimentation**\n\n**Sequential → Parallel → Hybrid Execution**\n\n**What roadblocks did I run into?**\n\n**Multi-Agent Orchestration Patterns**\n\n**Problem:** Choosing the right orchestration pattern for different scenarios - when to use delegation vs programmatic.\n\n**Pattern Complexity:**\n\n*   Single agent workflows (simple but limited)\n*   Agent delegation via tools (complex dependency management)\n*   Programmatic hand-off (coordination complexity)\n\n**→ Solution:** Hybrid approach combining multiple patterns:\n\n**What Worked Best?**\n\n*   **Best-of-N with LLM Judge**: High-quality, confidence-rated outputs\n*   **PydanticEvals**: Regression testing with real and mock models\n*   **Logfire Observability**: Debuggable and monitorable in \"production\"\n*   **Parallel Execution**: Critical speedup for real-time use\n*   **4o-mini**: Balanced cost and performance for most structured tasks\n\n**What's the smallest model that worked decently well?**\n\n*   **GPT-4o-mini:** Balanced performance (12.77s avg), good cost-value ratio.",
      "github_urls": [
        "**Git-Repo**](https://github.com/HendrikReh/rfq-multi-agent-system",
        "https://github.com/HendrikReh/rfq-multi-agent-system"
      ],
      "categories": [
        "Finance/Trading",
        "Classification/Analysis",
        "Evaluation/Benchmarking",
        "Web Application",
        "Multi-Agent Delegation",
        "Research/Experimentation",
        "Conversational AI",
        "Multi-Agent Orchestration"
      ],
      "technologies": [
        "PydanticAI",
        "Logfire",
        "OpenAI"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 2197,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": true,
        "mentions_results": true,
        "mentions_future_work": false,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 1.0,
        "completeness_score": 1.0,
        "overall_quality": 1.0
      },
      "created_at": "2025-06-22T13:49:27.225793Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {
        "heart": 4,
        "clap": 6,
        "fire": 1
      },
      "attachments_count": 1
    },
    {
      "user_name": "Jeff Fedor",
      "user_id": "cohort_20383_user_271220",
      "project_text": "**A11y Agent: AI-Powered Accessibility Improvement Agent**\n\n**Overview**\n\nMany years ago I used to write code, about 15 years ago made the switch to leadership, getting back into this but 100% used Cursor and Claude for this so no doubt there's cruft in the code. I learned a ton in this exercise and had way too much fun with Weave.\n\n**Task Overview**\n\nI chose to build an agent that automatically improves the accessibility of Markdown documents through multi-turn interactions. The agent converts Markdown to HTML, runs accessibility audits using Lighthouse and axe-core, then uses LLMs to fix identified issues iteratively until reaching a target accessibility score.\n\n**Agent Implementation**\n\n**Scaffold Architecture**\n\n*   Async-first design with AsyncA11yAgent using semaphores for rate limiting (5 concurrent LLM requests, 3 concurrent audits)\n*   Multi-tool integration: Lighthouse for overall accessibility scoring, axe-core for detailed WCAG violations\n*   LLM client supporting 8 different models via OpenRouter API\n*   Comprehensive logging with CSV results tracking, Weave monitoring, and session metrics\n\n**Evaluation & Reward Function**\n\nThe reward function combines two accessibility audit tools:\n\n*   Lighthouse score (60% weight): 0-1 score for overall accessibility\n*   Axe-core score (40% weight): Exponential decay based on violation count/severity\n\nThis dual approach caught issues that single-tool evaluation missed. Pure Lighthouse scoring initially produced \"obviously broken\" outputs, so adding axe-core violations provided the granular feedback needed for effective improvements.\n\n**Testing & Results**\n\nModel Performance\n\nTesting across 5 models showed:\n\nSmallest model that worked decently: mistralai/devstral-small:free\n\n*   Achieved perfect 1.0 score in just 2 turns (36.26s total)\n*   Free tier model, proving cost-effective solutions are viable\n\nMost efficient: meta-llama/llama-3-8b-instruct\n\n*   Reached 0.982 score in 2 turns with the fastest processing times\n*   Best balance of quality and speed\n\nStruggling model: google/gemma-3n-e4b-it:free\n\n*   Plateaued at 0.636 score despite 5 turns\n*   Couldn't resolve remaining accessibility issues\n\n**Key Findings**\n\n1.  Parallelization benefits: Running Lighthouse and axe-core concurrently reduced turn time by ~50%\n2.  Model strength correlation: Stronger models (Llama-3, GPT-4.1-nano, DeepSeek) consistently achieved >0.98 scores in 2 turns\n3.  Diminishing returns: Models either succeeded within 2-3 turns or plateaued, suggesting inherent capability limits\n\n**Roadblocks Encountered**\n\nMajor technical challenges were:\n\n1.  Lighthouse URL Restrictions: file:// URLs were rejected - implemented local HTTP server workaround\n2.  ChromeDriver Compatibility: Version mismatches between Chrome and ChromeDriver - solved with npx browser-driver-manager\n3.  Tool Documentation Gaps: axe-core CLI flags were outdated in examples - had to verify actual options\n\n**Lessons Learned**\n\n1.  Start with robust evaluation: Single-metric evaluation (Lighthouse only) was insufficient - combining multiple tools provided better agent guidance\n2.  Async architecture pays off: Despite added complexity, parallel processing significantly improved performance\n3.  Small models worked shockingly well\n4.  Tool integration is complex: Each tool brings unique requirements - version management, API changes, and output format variations require defensive programming\n\n**Results**\n\nGraphed the results for fun here [https://jfedor.github.io/a11y-agent/](https://jfedor.github.io/a11y-agent/)\n\nrepo available here too: [https://github.com/jfedor/a11y-agent/tree/main/a11y-agent](https://github.com/jfedor/a11y-agent/tree/main/a11y-agent)",
      "github_urls": [
        "https://github.com/jfedor/a11y-agent/tree/main/a11y-agent](https://github.com/jfedor/a11y-agent/tree/main/a11y-agent"
      ],
      "categories": [
        "Document Processing",
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Reinforcement Learning",
        "Web Application",
        "Single Agent Application"
      ],
      "technologies": [
        "OpenAI",
        "Anthropic"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 1,
        "description_length": 3702,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": false,
        "mentions_results": true,
        "mentions_future_work": false,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 0.8,
        "completeness_score": 1.0,
        "overall_quality": 0.9
      },
      "created_at": "2025-06-22T18:26:51.546369Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {
        "clap": 5
      },
      "attachments_count": 2
    },
    {
      "user_name": "Ty Bohan",
      "user_id": "cohort_20383_user_654493",
      "project_text": "**Code:** [https://gist.github.com/blankwall/db6ca413069093803b16404552eeef53](https://gist.github.com/blankwall/db6ca413069093803b16404552eeef53)\n\nThis project details the development and experimentation of a system designed to generate a 7-day kid-friendly meal plan by interacting with a recipe API. The core idea was to automate the selection of suitable recipes while ensuring they met specific criteria, particularly kid-friendliness and nutritional balance.\n\nApproaches Tried\n\nThe primary approach involved:\n\n1.  **Iterative API Calls:** For each day of the week, a Large Language Model (LLM) was prompted to select a meal category.\n    \n2.  **Custom JSON Extraction:** After the LLM provided a category, a custom JSON extraction method was used to parse the LLM's response.\n    \n3.  **Recipe API Integration:** The extracted category was then used to call an external recipe API (via call_tool(\"get_meal_from_category\", {\"category\": category})) to retrieve a meal.\n    \n4.  **Reward Function for Validation:** A custom reward_function was implemented to evaluate the suitability of the retrieved meal. This function assessed several factors:\n    \n    *   **Protein Content:** Checking for protein keywords in the meal name.\n        \n    *   **Dinner Appropriateness:** Ensuring the meal category was not \"dessert,\" \"side,\" \"starter,\" or \"breakfast.\"\n        \n    *   **Kid-Friendliness:** A check_kid_friendly function (though its implementation isn't fully detailed here, it's crucial for the overall goal) was used to rate the meal's appeal to children, likely by analyzing ingredients and general meal type.\n        \n    *   **Main Course Classification:** Granting bonus points for typical main course categories (e.g., beef, chicken, pasta).\n        \n    *   **Dinner-Appropriate Terms:** Looking for terms like \"roast,\" \"grilled,\" or \"stew\" in the meal name.\n        \n5.  **Retry Mechanism:** A get_validated_meal function was implemented with a retry logic. If a selected meal did not meet the validation criteria (score below 60), the LLM was prompted again with feedback, guiding it to select a more appropriate category.\n    \n\nRoadblocks Encountered\n\nThe biggest challenge encountered was **selecting the recipe itself** from the list returned by the API. Initially, the approach involved:\n\n*   **Random Selection:** The API returns a list of recipes, and a random choice was made. This often led to sub-optimal selections that didn't meet the \"kid-friendly\" or \"nutritionally balanced\" criteria.\n    \n*   **LLM-Driven Recipe Selection (Complication):** An attempt was made to pass the list of recipes to an LLM along with an exclusion list to find the best option. This approach proved to be **overly complicated** to implement and manage effectively within the existing framework. The complexity of having the LLM reason over multiple options and an exclusion list, while adhering to the desired output format, became a significant hurdle.\n    \n\nAnother notable roadblock was the **tool-calling aspect**. Rather than allowing the AI to fully drive the tool calling for each day's meal selection, a **loop for each day** was implemented. This suggests that achieving a fully autonomous, AI-driven tool calling process for the entire sequence was either not feasible with the chosen LLM or became too complex to control reliably within the project's scope.\n\n**Evaluation Methods**\n\nThe primary evaluation method was a **custom scoring function** embedded within the reward_function. This function calculated an \"overall satisfaction score\" for each recipe based on:\n\n*   **Standard parsing of text for proteins and ingredients:** This likely involved keyword matching for protein sources and potentially analyzing ingredient lists for elements that might make a meal less kid-friendly (e.g., spicy ingredients).\n    \n*   **Categorization checks:** Ensuring the meal fit appropriate dinner categories.\n    \n*   **Kid-friendliness rating:** A crucial component, which provided a specific score and reason based on whether the meal was \"Very Kid-Friendly,\" \"Somewhat Kid-Friendly,\" or \"Not Kid-Friendly.\"\n    \n\nThe reward_function returned a boolean is_valid flag and a numerical score, along with detailed feedback for each meal. This feedback was instrumental in guiding the retry mechanism and understanding why certain meals were rejected. The final evaluation summarized the total_score and average score for the entire 7-day plan, providing a quantitative measure of the plan's overall quality.\n\n**Smallest Model That Worked Decently Well**\n\nThe model that successfully achieved decent results was meta-llama/Meta-Llama-3-8B-Instruct. While other models like deepseek-ai/DeepSeek-V3-0324 and Qwen3-14B. were considered or experimented with, proved to be effective enough for this task. The current code uses meta-llama/Meta-Llama-3-8B-Instruct. The key was likely the robust prompt engineering and the iterative feedback loop, which compensated for the model's size.",
      "github_urls": [
        "https://gist.github.com/blankwall/db6ca413069093803b16404552eeef53",
        "https://gist.github.com/blankwall/db6ca413069093803b16404552eeef53](https://gist.github.com/blankwall/db6ca413069093803b16404552eeef53"
      ],
      "categories": [
        "Infrastructure/Tooling",
        "Research/Experimentation",
        "API/Backend Service",
        "Classification/Analysis",
        "Reinforcement Learning",
        "Web Application",
        "Evaluation/Benchmarking",
        "Education/Learning"
      ],
      "technologies": [
        "JavaScript"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 4987,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": true,
        "mentions_results": false,
        "mentions_future_work": true,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 0.8,
        "completeness_score": 1.0,
        "overall_quality": 0.9
      },
      "created_at": "2025-06-22T19:13:21.168184Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {
        "clap": 3
      },
      "attachments_count": 2
    },
    {
      "user_name": "Paul Oreto",
      "user_id": "cohort_20383_user_629656",
      "project_text": "I had limited time, so I played with the verifiers library, creating a multiturn environment. My biggest struggle was finding a task that I had data or could create data for, which was interesting and which even the smallest OpenAI models couldn't already solve out of the box. I played with text reversal, taking lines from Shakespeare and having the agent give the reversed text. gpt-4o-nano typically makes mistakes. In the multi-turn interaction, I had the system give a hint about the number of errors, but I'm not sure how useful that is for improving the performance.\n\nI also experimented with a tool calling example, but I didn't feel like I had great data for one.\n\nAll in all, I think my biggest issue is trying to understand how the multi-turn environment is supposed to work conceptually. What is an actual good use case? Let's say I want to make an agent that negotiates the price of something with another agent. How do I train that? What's the reward function?",
      "github_urls": [],
      "categories": [
        "Research/Experimentation",
        "Single Agent Application",
        "Infrastructure/Tooling",
        "Reinforcement Learning"
      ],
      "technologies": [
        "OpenAI"
      ],
      "quality_metrics": {
        "has_github": false,
        "github_count": 0,
        "description_length": 975,
        "has_attachments": false,
        "detailed_description": true,
        "mentions_technical_details": false,
        "mentions_challenges": true,
        "mentions_results": true,
        "mentions_future_work": false,
        "code_references": false,
        "structured_presentation": false,
        "documentation_score": 0.6,
        "completeness_score": 0.25,
        "overall_quality": 0.425
      },
      "created_at": "2025-06-22T20:56:58.337825Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 0
    },
    {
      "user_name": "Ed G",
      "user_id": "cohort_20383_user_674791",
      "project_text": "**Setup:** I implemented a simulator for the [secretary problem](https://en.wikipedia.org/wiki/Secretary_problem)\n\nRewards / Scores are determined by whether an agent is able to use a single tool with different parameters to interact with the scenario and select a good secretary.\n\nRepo here: [https://github.com/edgan8/prod-agents-class/tree/master/secagent](https://github.com/edgan8/prod-agents-class/tree/master/secagent)\n\nI used the openai agents sdk and compared gpt-4.1 variants and different prompts with a heuristic hand-coded agent.\n\n**Agent System Prompt Details:** I tried two system prompts with the same details on the scenario but concluding with different instructions.\n\n*   The _reasoning_ prompt says \"Please describe any reasoning before issuing tool calls.\"\n*   The _simple_ prompt says \"Your only task is to use the step tool to interact with the scenario, you do not need to converse with the user.\"\n\nI ran the simulator for 50 trials with different random seeds for each combination of model + prompt I tested. Average scores reported below.\n\n**Results**:\n\n*   Heuristic: 0.82\n*   gpt-4.1 reasoning prompt: 0.80\n*   gpt-4.1 simple prompt: 0.48\n*   gpt-4.1-mini reasoning prompt: 0.66\n*   gpt-4.1-mini simple: 0.51\n*   gpt-4.1-nano reasoning prompt: 0.35\n\n**Takeaways:**\n\n*   gpt-4.1 and gpt-4.1-mini and gpt-4.1-nano have substantially different agentic performance.\n*   Prompting models to reason before taking action makes a huge difference. I'm surprised this is not done by default. Careful evals over multiple trials were essential here.\n*   These models are quite slow overall: my heuristic is easily 100x faster.\n\n**Potential feedback:**\n\n*   I am evaluating agents not by their responses but by the end state of my simulator. Is this how things are commonly done?\n*   Is there a way to get non-reasoning openai agents to think by default before taking action without prompt engineering? Maybe this is an area where Claude is easier to work with.\n*   Even these non-reasoning models were very slow, ~1s per round-trip. This might be an area where a RL with non-LLM could yield much better results.",
      "github_urls": [
        "https://github.com/edgan8/prod-agents-class/tree/master/secagent](https://github.com/edgan8/prod-agents-class/tree/master/secagent",
        "https://github.com/edgan8/prod-agents-class/tree/master/secagent"
      ],
      "categories": [
        "Web Application",
        "Single Agent Application"
      ],
      "technologies": [
        "OpenAI",
        "Anthropic"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 2127,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": false,
        "mentions_challenges": true,
        "mentions_results": true,
        "mentions_future_work": false,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 0.8,
        "completeness_score": 0.75,
        "overall_quality": 0.775
      },
      "created_at": "2025-06-22T22:36:36.168514Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {
        "clap": 4
      },
      "attachments_count": 3
    },
    {
      "user_name": "Tejas Khot",
      "user_id": "cohort_20383_user_528360",
      "project_text": "I wanted to setup an environment and harness which would be amenable to doing tons of multi-turn + tool-use experiments. I started building out an intelligent code debugging agent that automatically fixes broken Python functions.\n\nGithub repo: [https://github.com/tejaskhot/patchwork/](https://github.com/tejaskhot/patchwork/tree/main)\n\n**📋 What I Built:**\n\n*   Agent scaffold with multiple specialized tools (debugger, linter, plot inspector, test harness)\n    \n*   Evaluation framework with test cases and scoring metrics\n    \n*   Tested across GPT-4.1, GPT-4.1-mini, and GPT-4.1-nano\n    \n\n**🔧 Building Experience:**\n\n*   Dataset\n    \n    *   This bit was tricky. I wanted a dataset which would demonstrate meaningful tool-use and yet won't be solved by the smallest models. To keep things simple, I had Sonnet-4 generate a few diverse examples for me in the schema I set. Examples covered data filtering/sorting, string grouping, matplotlib plotting, statistical outlier removal, and URL slug generation.\n        \n*   Tools\n    \n    *   This is where I ended up spending tons/most of my time. I wrote up my own tools and ran into a myriad of issues getting them to play well with the agent.\n        \n        *   Fun fact: I had to write a custom reward function to override how pylint scores the code because apparently it doesn't like one-liners and scores them 0/10.\n            \n    *   It was an interesting experience debugging the debugger tool, figuring out how to run code in a decently safe way for testing, not blow over the context by carefully filtering the execution system traces, etc.\n        \n    *   I built a tool registry with some helper methods including for discovery of tools and generating dynamic tool descriptions from their docstrings.\n        \n*   Prompting\n    \n    *   Keeping the system and user prompts to the point with explicit instructions and dynamically injecting the tool/problem context worked well. Probably could do more experimentation in the future as the task complexity increases.\n        \n*   Models\n    \n    *   Used LiteLLM as the main interface to keep open the possibility for testing many models easily.\n        \n    *   Ran tests using the GPT-4.1 series (nano, mini, large). The nano model was great for fast turnaround during debugging sessions.\n        \n*   Rewards\n    \n    *   Added 3 tiers of rewards\n        \n        *   Primary Deterministic Rewards (success rate, completion rate, efficiency score)\n            \n        *   Secondary Objective Rewards (invalid action penalty, regression penalty, linter score)\n            \n        *   Tertiary Subjective (aka LLM-as-judge) Rewards (code elegance score, strategic efficiency score)\n            \n    *   Didn't get around to playing with the calibration on these much, but there's plenty of room to tweak things here.\n        \n*   Evals\n    \n    *   Compared the selected model set on my tiny example dataset and unsurprisingly they all got every question correct. There are some differences in efficiency between the models as expected, but this needs a much larger dataset to glean any insights and potentially more realistic examples.\n        \n    *   I've implemented best-of-N sampling but need a more challenging dataset to make the most of it.\n        \n\n  \nI'd appreciate any comments, feedback, contributions or issues!",
      "github_urls": [
        "https://github.com/tejaskhot/patchwork/",
        "https://github.com/tejaskhot/patchwork/tree/main",
        "https://github.com/tejaskhot/patchwork",
        "https://github.com/tejaskhot/patchwork/](https://github.com/tejaskhot/patchwork/tree/main"
      ],
      "categories": [
        "Infrastructure/Tooling",
        "Evaluation/Benchmarking",
        "Reinforcement Learning",
        "Web Application",
        "Research/Experimentation",
        "Single Agent Application"
      ],
      "technologies": [
        "OpenAI",
        "Python"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 4,
        "description_length": 3342,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": true,
        "mentions_results": true,
        "mentions_future_work": true,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 1.0,
        "completeness_score": 1.0,
        "overall_quality": 1.0
      },
      "created_at": "2025-06-23T04:56:29.52156Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {
        "clap": 1
      },
      "attachments_count": 3
    },
    {
      "user_name": "Kelly",
      "user_id": "cohort_20383_user_665108",
      "project_text": "**Defi Activities Classifier**\n\nThis is the first AI model I’ve built, and I’d appreciate feedback on the strategy and evaluation approach. A detailed report is available in [git repo](https://github.com/yulongkelly/AI-agents/blob/main/Defi%20Activities%20Classifier%20Report.md)\n\n**Objective & Approach**\n\nThe system was designed to automatically analyze blockchain transactions and identify user intent patterns (staking, withdrawing, lending, borrowing, etc.) across various DeFi protocols. The implementation used custom AI agents with function calling capabilities to process transaction data fetched from APIs like DeFi Llama and Etherscan.\n\n**Two Strategies Tested (Both use GPT-4.1-mini)**\n\nStrategy 1: Single-Pass Protocol Analysis\n\nStrategy 2: Method-ID Specific Analysis\n\n**Key Results**\n\n**Rule Quality Evaluation:**\n\n*   Strategy 2 outperformed Strategy 1 in completeness and coverage\n    \n\n*   Strategy 1: 2/22 rules had missing fields, 1 missing rule\n    \n\n*   Strategy 2: 1/24 rules had missing fields, 0 missing rules\n    \n\n*   Accuracy for Strategy 2: 92.3% (60/65) when evaluated against human-created rules using GPT-4-turbo\n    \n\nNeither strategy produced duplicate rules\n\n**Main Challenges**\n\n*   Rate limiting constraints (GPT-4.1: 30K tokens/minute)\n    \n\n*   Maximum iteration limits requiring batch processing adjustments\n    \n\n*   Data quality issues with insufficient transaction samples for some method IDs\n    \n\n**Conclusion & Future Work**\n\nStrategy 2 proved superior for rule completeness. Future improvements include implementing asynchronous processing, testing different AI models, dynamic transaction fetching, and adopting Pydantic AI for better structured outputs.",
      "github_urls": [
        "repo](https://github.com/yulongkelly/AI-agents/blob/main/Defi%20Activities%20Classifier%20Report.md",
        "https://github.com/yulongkelly/AI-agents/blob/main/Defi%20Activities%20Classifier%20Report.md"
      ],
      "categories": [
        "Finance/Trading",
        "Classification/Analysis",
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Web Application",
        "Data Processing/ETL",
        "Single Agent Application"
      ],
      "technologies": [
        "PydanticAI",
        "OpenAI"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 1702,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": false,
        "mentions_results": true,
        "mentions_future_work": true,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 0.8,
        "completeness_score": 1.0,
        "overall_quality": 0.9
      },
      "created_at": "2025-06-23T05:07:39.855752Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {
        "clap": 2
      },
      "attachments_count": 1
    },
    {
      "user_name": "Morteza",
      "user_id": "cohort_20383_user_620493",
      "project_text": "**Rental Apartment Finding Agent - Project Introduction**  \n[https://github.com/mory91/rent-finder](https://github.com/mory91/rent-finder)\n\nOverview:\n\nI built a **Rental Apartment Finding Agent** that helps users find apartments through multi-turn conversations. The agent searches Craigslist, analyzes listings, and provides personalized recommendations based on user preferences.  \nKey Features\n\n*   **Multi-turn conversations** with tool integration\n    \n\n*   **Web scraping** of Craigslist listings\n    \n\n*   **Intelligent ranking** based on user preferences\n    \n\n*   **Comprehensive evaluation** system with multiple metrics\n    \n\n*   **Multi-model testing** (gpt-4.1-mini, gpt-4.1-nano)\n    \n\nTechnical Stack\n\n*   **Python 3.13** with async/await\n    \n\n*   **OpenAI/Anthropic APIs** for LLM integration\n    \n\n*   **aiohttp/BeautifulSoup** for web scraping\n    \n\n*   **Pydantic** for data validation\n    \n*   **Rich/Typer** for CLI interface  \n    \n\n**Approach**:  \nI wanted to learn all the pieces of creating an agent from scratch, so I **only used OpenAI API wrapper** and built everything else myself - no frameworks like LangChain or AutoGen.  \nMain Approaches\n\n1\\. **Pure API + Parsing Approach**\n\n*   Direct OpenAI API calls with custom prompt engineering\n    \n\n*   Manual HTML parsing of Craigslist results using BeautifulSoup\n    \n\n*   Custom JSON schema validation and data transformation\n    \n\n*   Everything is just **parsing and API calling**\n    \n\n2\\. **Async Architecture**\n\n*   Built parallel-friendly system with asyncio\n    \n\n*   Concurrent tool execution for faster searches\n    \n\n*   Proper session management and error handling\n    \n\n3\\. **Best-of-N Selection**\n\n*   Generate multiple responses for same query\n    \n\n*   Use evaluation metrics to rank and selectbest response\n    \n\n*   Helps identify most reliable agent outputs\n    \n\n4\\. **Evaluation Methods**\n\n*   **Rule-based**: Check criteria compliance, data accuracy\n    \n\n*   **LLM-as-judge**: Use OpenAI to evaluate response quality\n    \n\n*   **Formatting rewards**: Validate JSON structure compliance\n    \n\n**Ranking quality**: nDCG calculation for preference alignment\n\n**Roadblocks Encountered**\n\n1\\. **Tool Integration Challenges**\n\n*   Getting the model to properly call tools with correct JSON format\n    \n\n*   Models sometimes returned malformed tool calls or ignored tool instructions\n    \n\n*   Had to iterate on prompts to ensure consistent tool usage\n    \n\n2\\. **Query Quality Issues**\n\n*   Models generated poor search queries that returned irrelevant results\n    \n\n*   Needed to engineer prompts to generate specific, targeted Craigslist searches\n    \n\nRequired multiple iterations to get useful apartment listings  \n3\\. **Context Length Errors**\n\n*   System messages became too long with detailed instructions\n    \n\n*   Had to shorten context and optimize prompt structure\n    \n*   Removed verbose explanations to stay within token limits\n    \n\n**  \nEvaluation Methods Performance**\n\n*   **Rule-based evaluation** worked best - it was reliable, fast, and provided consistent metrics for criteria compliance and data quality. **LLM-as-judge** was useful but slower and more expensive. **Formatting rewards** helped ensure proper JSON output structure.\n    \n\n**Smallest Model Performance**\n\n*   **gpt-4.1-mini** was the smallest model that worked decently well. It successfully handled tool calling, generated good search queries, and produced structured outputs. The nano version struggled with consistency and schema compliance.",
      "github_urls": [
        "https://github.com/mory91/rent-finder",
        "https://github.com/mory91/rent-finder](https://github.com/mory91/rent-finder"
      ],
      "categories": [
        "Infrastructure/Tooling",
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Web Application",
        "Data Processing/ETL",
        "Conversational AI",
        "Single Agent Application"
      ],
      "technologies": [
        "JavaScript",
        "Python",
        "OpenAI",
        "Anthropic",
        "LangChain"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 3526,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": false,
        "mentions_results": true,
        "mentions_future_work": false,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 0.8,
        "completeness_score": 1.0,
        "overall_quality": 0.9
      },
      "created_at": "2025-06-23T10:58:11.426114Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {
        "clap": 4
      },
      "attachments_count": 2
    },
    {
      "user_name": "Isfandiyar Shaheen",
      "user_id": "cohort_20383_user_647712",
      "project_text": "I built financial analysis MCP Client-Server for analyzing financial PDFs from Pakistan Stock Exchange. It parses natural language queries, resolves partial company names to ticker symbols, and executes parallel dimensional searches across financial statements.\n\nI'd been working on this before, but re did bits of it using ideas from Lecture 1 and Lecture 2. End result was a simpler implementation with fewer lines of code and working better. Though it's still + will remain a work in progress.\n\nRepo: [https://github.com/ishaheen10/psxgpt](https://github.com/ishaheen10/psxgpt)  \n\n1.  What approaches did you try?\n    \n\nI used the Args, Returns, Example format to simplify my query parser. Earlier I was using a lot more regex / deterministic logic. Now I'm essentially prompting the LLM with an example + prompt to say give JSON in a given format.\n\nEarlier approach was doing a lot of if/then analysis on generating multiple queries, revised one uses a general approach to generate n queries based on input. Basically my vector index can't handle multiple tickers, time periods or statement types, so if the input is rather broad, it needs to be parsed.\n\n2.  What roadblocks did you run into?\n    \n\nClaude 4 has been my go to coding model, biggest roadblock is just keeping it on a leash and ensuring it doesn't do more things than I've asked. The other roadblock comes when say I want to do away with llama index, only then to find that now I need to use numpy to create a filtering mechanism. so I think the balance between not using libraries / frameworks vs using them.\n\n3.  Which evaluation methods worked best for your task?\n    \n\nI've been playing a lot with chunks retrieved relative to chunks used. Also calculating number of times the right query is assembled when a ticker is not used but a bank name is used. I've got scores for a few of these things that matter, but I haven't figured a comprehensive score card and how to keep improving it.\n\n4.  What's the smallest model that worked decently well?\n    \n\nI've run my code on Llama 3 8b instruct running locally and it works for query parsing. My final step of report assembly still needs Gemini 2.5 flash because sometimes a lot of chunks are used to prepare a full report and so context length issues arise. But when context length is not an issue I can make Llama 3 8b work.",
      "github_urls": [
        "https://github.com/ishaheen10/psxgpt](https://github.com/ishaheen10/psxgpt",
        "https://github.com/ishaheen10/psxgpt"
      ],
      "categories": [
        "Finance/Trading",
        "Document Processing",
        "Infrastructure/Tooling",
        "API/Backend Service",
        "Classification/Analysis",
        "Evaluation/Benchmarking",
        "Web Application"
      ],
      "technologies": [
        "JavaScript",
        "Google AI",
        "NumPy",
        "LlamaIndex",
        "Anthropic"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 2344,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": true,
        "mentions_results": true,
        "mentions_future_work": false,
        "code_references": true,
        "structured_presentation": false,
        "documentation_score": 0.8,
        "completeness_score": 1.0,
        "overall_quality": 0.9
      },
      "created_at": "2025-06-23T13:32:14.368591Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 2
    },
    {
      "user_name": "Zaur Rzakhanov",
      "user_id": "cohort_20383_user_647916",
      "project_text": "**Problem:** I need to collect data on corporate acquisitions for academic research in finance. Employing RA is not an option; M&A data is expensive. My knowledge of Python is standard for an econ/finance academic. I was very short on time last week and this weekend.\n\n**Solution:** Adopted Will Brown's agent script to collect data on acquisitions. Step 1: Asked Claude 4 Sonnet to modify agent script (see prompt below), Step 2: Claude developed the script, Step 3: used the script in Google collab to collect data. Total development time: 5 minutes.\n\n**Claude 4 Sonnet prompt:** \"I have a list of company names, and I would like to find out whether they were acquired at any time between 2012 and 2024. To do that I would like to modify attached script. Your thoughts?\"",
      "github_urls": [],
      "categories": [
        "Web Application",
        "Finance/Trading",
        "Research/Experimentation",
        "Single Agent Application"
      ],
      "technologies": [
        "Anthropic",
        "Python"
      ],
      "quality_metrics": {
        "has_github": false,
        "github_count": 0,
        "description_length": 772,
        "has_attachments": false,
        "detailed_description": true,
        "mentions_technical_details": false,
        "mentions_challenges": true,
        "mentions_results": false,
        "mentions_future_work": false,
        "code_references": false,
        "structured_presentation": true,
        "documentation_score": 0.6,
        "completeness_score": 0.25,
        "overall_quality": 0.425
      },
      "created_at": "2025-06-23T13:33:55.779891Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 0
    },
    {
      "user_name": "James T",
      "user_id": "cohort_20383_user_662398",
      "project_text": "This message was deleted.",
      "github_urls": [],
      "categories": [
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Web Application",
        "Education/Learning",
        "Conversational AI",
        "Single Agent Application"
      ],
      "technologies": [
        "React",
        "AWS"
      ],
      "quality_metrics": {
        "has_github": false,
        "github_count": 0,
        "description_length": 9029,
        "has_attachments": false,
        "detailed_description": true,
        "mentions_technical_details": false,
        "mentions_challenges": true,
        "mentions_results": true,
        "mentions_future_work": false,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 0.8,
        "completeness_score": 0.5,
        "overall_quality": 0.65
      },
      "created_at": "2025-06-23T18:32:07.182124Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 0
    },
    {
      "user_name": "James T",
      "user_id": "cohort_20383_user_662398",
      "project_text": "**Report on Making a LessWrong Search Agent**\n\nGoal: Make a conversational agent with search tools over a downloaded repository of all LessWrong.\n\nIdeally, it should be able to accurately answer simple, very fact-based questions (\"What's the most popular post on LessWrong?\", \"What kind of things does user X like to talk about?\") but also more vague, subjective questions (\"How has Yudkowsky's emotional state changed for the worse over time?\")\n\nI'll divide the report into three sections: (1) problems I ran into spinning up the agent initially, before putting together the test questions, (2) performance and problems when testing on my first set of golden answers, and (3) performance and problems after revising the agent scaffolding in light of the problems encountered in section 2.\n\n**1\\. Initial Work**\n\nFor the chat engine, I'm using the basic ReAct pattern inside XML tags. The system prompt asks for text to all be in , , or tags. Not using any inference forcing; just returning an error to the LLM in case it doesn't return within this criteria.\n\nI started off with 4 LessWrong specific tools:\n\n*   A search tool over usernames, returning user names, average karma, most recent posts, etc.\n*   A search tool over posts, that lets you filter by user name and date, while returning post title, post id, etc.\n*   A tool that returns the first N characters for some post id.\n*   A tool that let's you query another LLM instance, to summarize some post or return specific points about it.\n\nInitial notes:\n\n*   Used DeepSeekV3-0528 for the base LLM.\n*   I had to adjust the system prompt to prevent hallucinated tool responses. I'd see the model try to do a tool-call inside , then hallucinate a response to the tool-call immediately afterwards. Pretty persistent across models without some edits.\n*   Found tests pretty important for tools, decided (after spinning up very buggy working version) to ofc write those first, to pretend that I wasn't a moron.\n*   LLMs seem lazy, or apt to do the minimum number of tool calls. At first, it was very inclined to just have _ONE_ tool use. After I changed the prompt to explicitly indicate multiple tool uses are allowed and encouraged, it would then do multiple calls. But if, for instance, you ask for a summary of what some dude on LW tends to write about, it still seems to prefer to do a search for a bunch of post titles, and then just summarize this content while inferring the content from the titles. Or, if you ask for what some author X has written about topic Y, it looks at the top 5 posts by that person and just summarizes from whichever of the top posts is MOST relevant to Y without bothering to search for more. I've fiddled with the system prompt but this seems like a recurring theme.\n*   I've added, to the tool descriptions for the LLMs, hints about when to use the tools. This has helped stop some dumb problems.\n*   tried using RegEx or string similarity functions for my evaluation questions. However, I quickly decided that for most questions these sucked. So I quickly switched to exclusively using LLM-as-a-judge, with an \"expected\" golden answer compared to the answer that the LLM returned.\n\n**2\\. Tests on 9 questions.**\n\nI put together 9 reasonably high quality questions with a LLM judge, that compares to a golden answer and then gives a score of correct or not. Here are the basic results on a variety of increasingly\n\n\\- DeepseekV3-0528: 8/9\n\n\\- Qwen_285: 5/9\n\n\\- Qwen_30b: 5/9\n\n\\- Llama3.3 70b: 2/9\n\nNotes on reading through the chain-of-thought for all these questions (see backend/test_results for the chain-of-thought / tool use invocations from each model).\n\n*   On long chains of thought requiring multiple tool uses, many LLMs seem apt to forget the format -- i.e., they will put their final response outside of a tag, in just plaintext after the tag. I've also noticed this in conversations through my UI. Even the smartest models seem apt to have this problem after long-enough of a problem. Probably just because the tag is standard but is not.\n*   There's a little bit of a problem with the LLMs like Qwen simply one-shotting LessWrong usernames from memory. Or, it's not exactly a problem, but it does make me distrust how good my tools and test would generalize to less common LessWrong usernames or cases.\n*   Curiously, Qwen also hallucinated errors from the tools.\n*   Llama3.3 70b has much more difficulty inferring whether particular tool calls should have integer or string arguments. I was irritated at Llama initially, but looking through my system prompt, I did note that I didn't have an example of tool calls involving integer values. So, mostly on me.\n\n**3\\. Changes in response to errors and results afterwards**\n\nChanges:\n\n*   Add example tool calls to the system prompt for each tool. In cases of complex tools, I include both a minimalistic prompt (only using mandated arguments) and a maximalistic prompt (using the maximum number of arguments possible) among the examples.\n*   Improving error messages returned to the LLM, so that if you tried to return text outside of XML tags OR if you tried using multiple XML tags of one type, the LLM gets a more informative LLM response in return. This is important, because rather than using inference formatting I am just letting the LLM work unconstrained.\n*   Dropping one tool -- the one that let you read the first N characters of a LessWong post -- in favor of exclusively permitting the use of a tool that allowed you to request another LLM for a summary. In general the post content-returning tool just sucked; LessWrong posts can be very long, and the first N characters often don't tell you that much.\n*   Increasing the maximum number of tool-calls from 10 to 32.\n*   Adding a final sentence to the system prompt that tells the LLM: \"When answering, think carefully about the meaning of the question. Feel free to think for a while, or use as many tool calls as you need to get the best answer.\"\n\n  \n\nThis gives us:\n\n\\- DeepseekV3-0528: 8/9 (?!)\n\n\\- Qwen3 30b: 7/9\n\n\\- Qwen3 235b: 7/9\n\n\\- Llama3.3 70b: 5/4\n\nExact numbers depend on luck, but if you repeat a few times you usually get numbers about like this.\n\nLooking through the results:\n\n*   I take back what I said about Llama 3.3 70b calling in the wrong format being my fault. Even when provided with explicit examples of the right format it wants to call in the wrong way. It's just stupid.\n*   In general, glancing at several runs, the LLMs struggle with queries that require you to recognize when some criteria has NOT been met. For instance, one of my questions asks for the most popular post by Paul F Christiano that is NOT a response to some other post. The most popular post by Christano, however, \"Where I agree and disagree with Elizer,\" and is a response to Yudkowsky. Every LLM had problems consistently noticing this; many read the title and said something like \"Well, it's not explicitly Re: What You Said,\" and rather than checking the post text decided to just give that as a response.\n*   Similarly, one of my questions asks for an imaginary concept -- \"Montaigne-influenced rationality\" -- and expects the LLM to respond be saying \"I couldn't find it.\" But LLMs prefer to confabulate probable answers (\"Well, Yudkowksy talks about rationality a lot, so he probably talked about this\") or just mention anyone who mentioned Montaigne. Again, they struggle a great deal with the negative; knowing when some query has NOT returned something which meets their goals.\n\n  \n\nLooking back, at things I should have spent much more time on from the start:\n\n*   My auto-judge sometimes returns tendentious yes or nos. In general, spending more time on my auto-judge would have probably been a good use of time.\n*   Probably more time making the conversation logic shorter and easier to understand would have paid over with a speedier testing cycle.\n*   Maybe larger number of simpler questions would have been better than the more advanced questions that I looked to evaluate at first.\n\n  \n\nBut in the end, the bot does work, and my UI does work. I'm reasonably happy that I can ask a question like this:\n\n\\> Could you try to summarize how Yudkowsky's style of posts have changed over time on LessWrong? Specifically, focus on how he's trying to evoke specific emotions in the reader, and how his relation to the reader and what he wants to do for them changes -- how does he see his relation to the reader? What kind of a follower does he want? Try to read at leas like, 4 early posts and 4 later posts.\n\n  \n\nAnd get an answer including this:\n\n  \n\n\\> Key Shifts:\n\n\\> 1. From Teaching to Warning: Early posts focus on training rationalists; later posts scream alarms about existential risks.\n\n\\> 2. From Community-Building to Polarization: \"Preface\" praises collective progress, but \"AGI Ruin\" dismisses mainstream efforts as suicidal.\n\n\\> 3. From Optimism to Fatalism: Early work assumes rationality can fix human flaws; later work implies most are doomed unless they meet his epistemic standards.",
      "github_urls": [],
      "categories": [
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Web Application",
        "Education/Learning",
        "Conversational AI",
        "Single Agent Application"
      ],
      "technologies": [
        "React",
        "AWS"
      ],
      "quality_metrics": {
        "has_github": false,
        "github_count": 0,
        "description_length": 9029,
        "has_attachments": false,
        "detailed_description": true,
        "mentions_technical_details": false,
        "mentions_challenges": true,
        "mentions_results": true,
        "mentions_future_work": false,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 0.8,
        "completeness_score": 0.5,
        "overall_quality": 0.65
      },
      "created_at": "2025-06-23T18:34:52.376418Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 0
    },
    {
      "user_name": "Louk de Loijer",
      "user_id": "cohort_20383_user_648978",
      "project_text": "**Goal:**\n\nThe task I focussed on is creating a SQL Agent that answers questions about a database with multiple tables. The information in the database is about a grid operator company (synthetically created).\n\n*   Agent framework: smolagents\n*   LLMs: _Mistral-medium_, _Mistra-small_ (API based) Database: DuckDB\n*   Available tools:\n*   Duckdb built-in converted to tools: \\[list_databases, list_schemas, list_tables, list_views, list_columns, list_dependencies\\]\n*   Created tools: \\[describe_table, query_db\\]\n\n**Github repository:** [link](https://github.com/loukdeloijer/agent-engineering/tree/main/homework-1)\n\n**consistent problem: sometimes it's the tool not the agent**\n\nInitially the agent seemed to ignore the output of the tools it called. An example of this is the query_db tool, where it tried to determine the existence of transmission line with the ID 2. The logs would display that lineid_ 2 existed but Execution logs would show: None resulting in the final answer: \"There is no maintenance history available for line ID 2…\"\n\nAfter some debugging and using the tool myself I found that even though the .show in the tool would display the rows, the output of the tool was NoneType. Each time the query_db tool was called, the agent would think the tool returned nothing instead of the rows of the table.\n\nLesson learned: test your tools!\n\n**Broader evaluation**\n\nI created a small set of 5 test prompts and evaluated the agent with two metrics:\n\n*   LLM Judge: score 1 if the answer is correct, 0 otherwise\n*   Embedding similarity: score based on the cosine similarity of the embeddings of the generated answer and the model answer\n\nConclusion: generated answers were too long for embedding similarity to be a good metric.\n\n**Scores**\n\n**Mistral-small**:\n\n*   LLM Judge: 0.2 -> only 1 correct out of 5\n*   Embedding similarity: average dot product of 0.8 -> not a good eval.\n\n**Mistral-medium**:\n\n*   LLM Judge: 1 -> all correct\n*   Embedding similarity: average dot product of 0.89 -> not a good eval.\n\n**Observations**\n\n*   Quite often the agent would write python code along with the SQL tools, which was not expected. Is this desirable?\n*   Quite often it tried to import pandas to process the results of the query_db tool, which was not expected. Is this desirable?\n*   If you don't limit the max_steps of an agent, token usage can build up quickly.\n*   Example: Step 20: Input tokens: 209,148 | Output tokens: 5,148\n*   Mistral-medium takes significantly less tool calls than Mistral-small. ~3 average tool calls for Mistral-medium vs ~12 for Mistral-small. This is because Mistral-medium is better at calling the query_db tool with the correct SQL query.\n*   Even though embedding similarity of Mistral-medium is higher than Mistral-small, I would argue that its too close for in comparison to the differences in the LLM judge score. As the metrics don't agree, one of the metrics is not a good metric. Clearly this is the embedding similarity metric.\n\n**Future work**\n\n*   I haven't evaluated the SQL agent on the actual SQL it generated, which could be included in the reward function.\n*   Would it be good to reward the agent taking less tool calls to come up with the correct answer? In an RL setting, this could discourage the agent from using tools.\n\nFeedback is always welcome! (Especially on what other rewards I could have used for this agent)",
      "github_urls": [
        "link](https://github.com/loukdeloijer/agent-engineering/tree/main/homework-1",
        "https://github.com/loukdeloijer/agent-engineering/tree/main/homework-1"
      ],
      "categories": [
        "Infrastructure/Tooling",
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Reinforcement Learning",
        "Web Application",
        "Single Agent Application"
      ],
      "technologies": [
        "Pandas",
        "Python"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 3379,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": true,
        "mentions_results": false,
        "mentions_future_work": true,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 0.8,
        "completeness_score": 1.0,
        "overall_quality": 0.9
      },
      "created_at": "2025-06-23T19:38:39.434316Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 1
    },
    {
      "user_name": "Johnny Heo",
      "user_id": "cohort_20383_user_542001",
      "project_text": "I made a general-purpose markdown editor CLI agent based on claude code.\n\nInitially tried going with OpenAI and working in a python notebook, but I thought working on a CLI agent would be more interactive and educational.\n\nSome roadblocks.. it took me some time to get familiar with Logfire and implementing it into an agent to get visibility into each tool call. I started with some pretty simple tools, but then I decided to implement sub-agents and the todo list and that's where it got tricky.\n\nFor evals, I started off with simple similarity scores between expected and actual outputs, but that wasn't working well for complex multi-step prompts. I ended up using a LLM judge (Haiku) to evaluate the tool calls and tool outputs. It was cool to see Haiku commenting on how the tool outputs could be more descriptive because I could use that feedback to improve the tool definitions and run the evals again to keep iterating. Love a good feedback loop!\n\nI decided not to experiment with local models on this go, so Haiku 3.5 is the smallest model I used.\n\n**Sonnet 4**\n\n*   **Average Score:** 82.7%\n    \n*   **Efficiency Rate:** 93.75%\n    \n*   **Quality Distribution:**\n    \n    *   Excellent: 7 responses\n        \n    *   Good: 6 responses\n        \n    *   Fair: 2 responses\n        \n    *   Poor: 0 responses\n        \n\n**Haiku 3.5**\n\n*   **Average Score:** 81.9%\n    \n*   **Efficiency Rate:** 100%\n    \n*   **Quality Distribution:**\n    \n    *   Excellent: 9 responses\n        \n    *   Good: 4 responses\n        \n    *   Fair: 3 responses\n        \n    *   Poor: 0 responses\n        \n\n**Performance Comparison**\n\n*   Sonnet 4 has a slightly higher average score (82.7% vs 81.9%)\n    \n*   Neither model produced any poor quality responses\n    \n\n**Quality Distribution**\n\n*   **High-quality responses (Excellent + Good):**\n    \n    *   Sonnet 4: 86.7% (13/15)\n        \n    *   Haiku 3.5: 81.3% (13/16)\n        \n\nI plan to keep building on top of it and build more complex writing and reading tools :)",
      "github_urls": [],
      "categories": [
        "Finance/Trading",
        "Research/Experimentation",
        "Web Application",
        "Education/Learning",
        "Conversational AI",
        "Single Agent Application"
      ],
      "technologies": [
        "Logfire",
        "OpenAI",
        "Anthropic",
        "Python"
      ],
      "quality_metrics": {
        "has_github": false,
        "github_count": 0,
        "description_length": 2003,
        "has_attachments": false,
        "detailed_description": true,
        "mentions_technical_details": false,
        "mentions_challenges": false,
        "mentions_results": true,
        "mentions_future_work": true,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 0.6,
        "completeness_score": 0.5,
        "overall_quality": 0.55
      },
      "created_at": "2025-06-23T20:24:00.491076Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {
        "clap": 1
      },
      "attachments_count": 0
    },
    {
      "user_name": "Predrag Malicevic",
      "user_id": "cohort_20383_user_375097",
      "project_text": "I made an agentic RAG example where the agent is able to discover different data sources and run queries against them:\n\n  \n\n[https://github.com/pmalic/prae-agent-prototype](https://github.com/pmalic/prae-agent-prototype)\n\n  \n\nI didn't spend a lot of time so added support for just one data source type (SQLite), but the existing \"architecture\" allows adding new data sources pretty easily. My evals \"suite\" is also very small, just two question/answer pairs, but it shows the idea.\n\n  \n\nAlso have a reward function that \"penalizes\" the agent for running too many turns (\"action steps\" in smolagents).\n\n  \n\nOnly tried GPT 4.1 models, \"normal\" and -mini work OK, -nano can't always figure out how to call the tools properly.\n\n  \n\nOne thing I noticed but didn't have time to play with: I have a tool that can return the description of some data source (kept in an .md file) and there's also a tool for retrieving the database schema for SQLite data sources. No model I tried wants to call the first tool to read the description of the data source, only calls the schema tool and then the query tool. The data source description could contain some useful info how to query the database properly, so should be read by the agent. An explicit \"advice\" to call it is probably needed in the tool's docstring. The reward function could also penalize the agent for not calling this tool first.\n\n  \n\nThe quickest way to \"scan\" the prototype is to go through this notebook: [https://github.com/pmalic/prae-agent-prototype/blob/master/notebook.ipynb](https://github.com/pmalic/prae-agent-prototype/blob/master/notebook.ipynb)",
      "github_urls": [
        "https://github.com/pmalic/prae-agent-prototype/blob/master/notebook.ipynb](https://github.com/pmalic/prae-agent-prototype/blob/master/notebook.ipynb",
        "https://github.com/pmalic/prae-agent-prototype/blob/master/notebook.ipynb",
        "https://github.com/pmalic/prae-agent-prototype](https://github.com/pmalic/prae-agent-prototype",
        "https://github.com/pmalic/prae-agent-prototype"
      ],
      "categories": [
        "Web Application",
        "Research/Experimentation",
        "Single Agent Application",
        "Reinforcement Learning"
      ],
      "technologies": [
        "SQLite"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 4,
        "description_length": 1611,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": false,
        "mentions_results": false,
        "mentions_future_work": false,
        "code_references": true,
        "structured_presentation": false,
        "documentation_score": 0.4,
        "completeness_score": 1.0,
        "overall_quality": 0.7
      },
      "created_at": "2025-06-23T20:56:17.090593Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 4
    },
    {
      "user_name": "Aswin Bhaskaran",
      "user_id": "cohort_20383_user_474582",
      "project_text": "**Goal:** To develop a tax-focused AI agent capable of not only answering queries by retrieving relevant information from custom documents but also dynamically making external API calls based on context derived from those documents. The solution leverages the ReAct agent framework, integrating multiple tools and contextual understanding to enhance response accuracy. Custom documents—such as the OECD Transfer Pricing Guidelines and Form 990 instructions—are loaded and indexed to support domain-specific queries. Additionally, the concept of an Agentic Router for Retrieval-Augmented Generation (RAG) was explored. The agent’s performance was evaluated using the RAGAS framework with a small sample dataset. A reward function to evaluate the quality of the retrieved context by text by comparing it to known ground truth from sample dataset and also tried reward function using cosine similarity.\n\nTechnology:\n\nlangchain\n\nllama-index\n\nLLM: gpt-4o-mini\n\nEmbeddings :  text-embedding-3-small\n\nRagas: Evaluating responses from Agent with small sample dataset.\n\nExternal API: **ProPublica Nonprofits API:** An external API used as a tool for the agent.\n\n  \n  \nDeliverable:  \nBunch of notebooks trying to figure things out—sorry if I’m way off.  \n[aswinaus/Assignments: Assignments](https://github.com/aswinaus/Assignments)",
      "github_urls": [
        "Assignments](https://github.com/aswinaus/Assignments",
        "https://github.com/aswinaus/Assignments"
      ],
      "categories": [
        "Document Processing",
        "Infrastructure/Tooling",
        "API/Backend Service",
        "Reinforcement Learning",
        "Web Application",
        "Single Agent Application"
      ],
      "technologies": [
        "React",
        "LangChain",
        "OpenAI"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 1321,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": false,
        "mentions_results": true,
        "mentions_future_work": true,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 0.8,
        "completeness_score": 1.0,
        "overall_quality": 0.9
      },
      "created_at": "2025-06-24T00:33:23.936069Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 1
    },
    {
      "user_name": "Diana Padilla",
      "user_id": "cohort_20383_user_660063",
      "project_text": "Hi guys!\n\nThis is an [ongoing project](https://github.com/dcpadilla01/podcast_agent). I wanted to do an agent that could \"talk to my podcasts\". There are many I listen to and there's just so many hours in the day.\n\n1.  Refer to an RSS Feed\n    \n2.  A directory of episodes will be created along with a local Chroma DB of the podcast\n    \n3.  Download a desired episode\n    \n4.  Gemini transcribes it with timestamps (the timestamps are done by the program, not by Gemini)\n    \n5.  Transcription chunks are saved into a local Chroma DB\n    \n\nNow, with LangGraph, you can ask the agent to play the podcast (via text).\n\nIt's in WIP. I still have to give to the agent the following abilities:  \n  \n1\\. Download episode\n\n2.  Control the VLC player\n    \n    1.  Pause (right now it can only play the episode)\n        \n    2.  Play from timestamp, which allows you to look in your podcasts (the ones that are already transcribed) and play from there.\n        \n\nThanks!",
      "github_urls": [
        "project](https://github.com/dcpadilla01/podcast_agent)",
        "https://github.com/dcpadilla01/podcast_agent"
      ],
      "categories": [
        "Single Agent Application"
      ],
      "technologies": [
        "Google AI"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 961,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": false,
        "mentions_challenges": false,
        "mentions_results": false,
        "mentions_future_work": false,
        "code_references": true,
        "structured_presentation": false,
        "documentation_score": 0.2,
        "completeness_score": 0.75,
        "overall_quality": 0.475
      },
      "created_at": "2025-06-24T00:57:07.097434Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {
        "clap": 1
      },
      "attachments_count": 1
    },
    {
      "user_name": "Skylar Payne",
      "user_id": "cohort_20383_user_482225",
      "project_text": "I made an agent that conducts me a weekly review... I even gave a Lightning lesson on it. Slides below:\n\nlightning lesson link: [https://maven.com/p/05b8f8/10x-your-productivity-by-building-personal-agents-with-mcp](https://maven.com/p/05b8f8/10x-your-productivity-by-building-personal-agents-with-mcp)\n\n[https://docs.google.com/presentation/d/1KCodHNEbq-83wa-Ixp-Tg5WxRCqmhSCkDAmAug0pCBc/edit?slide=id.g3461752da27_0_25#slide=id.g3461752da27_0_25](https://docs.google.com/presentation/d/1KCodHNEbq-83wa-Ixp-Tg5WxRCqmhSCkDAmAug0pCBc/edit?slide=id.g3461752da27_0_25#slide=id.g3461752da27_0_25)",
      "github_urls": [],
      "categories": [
        "Web Application",
        "Single Agent Application"
      ],
      "technologies": [],
      "quality_metrics": {
        "has_github": false,
        "github_count": 0,
        "description_length": 592,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": false,
        "mentions_challenges": false,
        "mentions_results": false,
        "mentions_future_work": false,
        "code_references": false,
        "structured_presentation": false,
        "documentation_score": 0.2,
        "completeness_score": 0.25,
        "overall_quality": 0.225
      },
      "created_at": "2025-06-24T01:06:48.126875Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {
        "fire": 1
      },
      "attachments_count": 4
    },
    {
      "user_name": "Zane Peycke",
      "user_id": "cohort_20383_user_646768",
      "project_text": "[https://github.com/ZanePeycke/prae](https://github.com/ZanePeycke/prae)\n\n**Task: Personalized broker to find apartments in New York City**\n\n*   Agent has access to tools and personal details to find the best apartment for each person.\n    \n*   Agent can parse listing and go beyond StreetEasy filters, better understand the renter through large amounts of text, images, etc. (eventually could proactively reach out to schedule viewings)\n    \n*   Reward function is if the person want's to schedule a viewing (determined by LLM as a judge with personal info for the time being)\n    \n\n**Details**\n\n*   Chat completions API with 4.1 or sonnet for the main model\n    \n    *   4o for parsing images, openai web search\n        \n*   Tools:\n    \n    *   Apartment Text search\n        \n    *   Analyze apartment images\n        \n    *   Explore neighborhood\n        \n    *   Map directions\n        \n*   Outcomes:\n    \n    *   Recommend apartment(s)\n        \n    *   Do nothing\n        \n\n**What I would like to improve on**\n\n*   Better file search: Should gather more data and consider organizing it by neighborhood, provide more context on the schema\n    \n*   Better search by keywords (a budget of 4000 should include listings less than 4k but now fails because i'im comparing on ints)\n    \n*   Larger apartment dataset for testing\n    \n*   Analyze tool usage\n    \n*   Re add [Open Street Map MCP](https://github.com/jagan-shanmugam/open-streetmap-mcp) for better directions and explorations\n    \n*   Error handling on context window limits for Claude\n    \n*   Larger Test Set , Async eval, store evals",
      "github_urls": [
        "https://github.com/ZanePeycke/prae](https://github.com/ZanePeycke/prae",
        "https://github.com/ZanePeycke/prae",
        "https://github.com/jagan-shanmugam/open-streetmap-mcp",
        "MCP](https://github.com/jagan-shanmugam/open-streetmap-mcp"
      ],
      "categories": [
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Reinforcement Learning",
        "Conversational AI",
        "Single Agent Application"
      ],
      "technologies": [
        "OpenAI",
        "Anthropic"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 4,
        "description_length": 1594,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": false,
        "mentions_results": false,
        "mentions_future_work": true,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 0.6,
        "completeness_score": 1.0,
        "overall_quality": 0.8
      },
      "created_at": "2025-06-24T02:59:38.083752Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 3
    },
    {
      "user_name": "Jacob Milleville",
      "user_id": "cohort_20383_user_647925",
      "project_text": "I built a hierarchical research agent with specific tooling for searching PubMed articles and/or the web to answer health-related questions. It attempts to use task classification to branch into a subtask/subagent tree (similar to what I envisioned being described in Anthropic's multi-agent blog post for their research agent).  \n  \nRepo: [https://github.com/jake-milleville/health-research-agent-demo](https://github.com/jake-milleville/health-research-agent-demo)  \n  \nI didn't have enough time to polish everything as much as I wanted, especially the prompting, but the basic approach looks like:  \n  \n**Hand-rolled framework**: no agent libraries or external frameworks for maximum flexibility and extensibility  \n**Task decomposition**: LLM classifier handles each task in advance, branching into subtasks when needed  \n**Agent tree**: each agent collects inputs from its children before handling its own task, synthesizing downstream results  \n**Parallel execution**: subagents run concurrently - parents block their own tasks until children callback, but each tree runs independently.  \n**Custom tool**: Uses a PubMed API library as a tool to enable searching for quality research. Also has some default tools lifted from the agent lightning lesson a while ago for searching the web and fetching/summarizing page results.  \n  \n  \n**Model Comparison**: I went cheap and was using free/cheap models through OpenRouter. Mistral Small 3.2b worked fairly well but kind of sloppy and seemed to be not super adept at using custom tools effectively. R1 Distill Qwen 7b was ok, much smarter but provider was very slow so couldn't get great tests. Gemini 2.5 Flash Lite 6-17 preview was quite good - very fast and a natural with tool use. Would definitely be ok building with it which is nice given price/speed. Didn't try anything bigger but I'm sure flagship models would crush it, especially given that a lot of the new ones are already so good at research out of the box.  \n  \nI didn't get a chance to set up an eval workflow so all the model judgements were vibes only, but I am excited to hack on this later and at the very least I like having a reference setup for an agent without much overhead!",
      "github_urls": [
        "https://github.com/jake-milleville/health-research-agent-demo](https://github.com/jake-milleville/health-research-agent-demo",
        "https://github.com/jake-milleville/health-research-agent-demo"
      ],
      "categories": [
        "Finance/Trading",
        "Infrastructure/Tooling",
        "API/Backend Service",
        "Classification/Analysis",
        "Web Application",
        "Healthcare/Medical",
        "Research/Experimentation",
        "Multi-Agent Orchestration"
      ],
      "technologies": [
        "Google AI",
        "Anthropic"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 2201,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": false,
        "mentions_results": false,
        "mentions_future_work": false,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 0.6,
        "completeness_score": 1.0,
        "overall_quality": 0.8
      },
      "created_at": "2025-06-24T03:05:21.050775Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 2
    },
    {
      "user_name": "Fernando Meira",
      "user_id": "cohort_20383_user_572657",
      "project_text": "[GitHub repo](https://github.com/fmeiraf/deep-leads)\n\n  \n\n**Deep Lead Researcher**\n\n  \n\nI decided to build a **Lead Search** agent that uses LLM agents to systematically research and extract high-quality professional contacts based on specific search criteria.\n\nReasons why I picked this project:\n\n\\- This was something I wanted for another project I am working on.\n\n\\- I wanted something that had very simple and composable tools but that could scale chaining them properly (so the challenge of optimizing gets more interesting).\n\n\\- I was particularly interested in testing this after reading the [latest Anthropic article on **multi agents**](https://www.anthropic.com/engineering/built-multi-agent-research-system).\n\n  \n\n**Technical Stack**\n\n\\- **Agent Framework**: PydanticAI for structured agent interactions\n\n\\- **Web Research**: Tavily API for intelligent web search\n\n\\- **LLM Models**: OpenAI GPT-4.1 series, Anthropic Claude series\n\n\\- **Evaluation**: DeepEval for automated assessment\n\n\\- **Visualization**: Rich library for terminal-based result presentation\n\n\\- **Data Models**: Pydantic for type-safe data structures\n\n  \n\n**Core Components**\n\n  \n\n**1\\. Structured Data Models (\\`src/types.py\\`)**\n\n\\- **\\`ResearchParams\\`**: Defines search parameters using WHO/WHAT/WHERE/CONTEXT framework\n\n\\- **\\`Lead\\`**: Comprehensive contact model with validation and string representation\n\n\\- **\\`LeadResults\\`**: Container for multiple leads with aggregation methods\n\n\\- **\\`EvalParams\\`**: Evaluation framework combining query parameters with expected results\n\n  \n\n**2\\. Multi-Pattern Agent Architecture (\\`src/agents/\\`)**\n\n  \n\n**Single Agent Pattern** (\\`single_agent_pattern.py\\`):\n\n\\- Comprehensive 252-line system prompt with detailed search methodology\n\n\\- 3-phase research process: Discovery → Site Mapping → Extraction\n\n\\- Built-in error handling and parallel tool execution\n\n\\- Strict anti-hallucination constraints\n\n  \n\n**Multi-Agent Pattern** (\\`multi_agent_pattern.py\\`):\n\n\\- Orchestrator-Researcher coordination system\n\n\\- Strategic task decomposition and delegation\n\n\\- Parallel execution across multiple specialized researchers\n\n\\- Enhanced quality control through distributed verification\n\n  \n\n**3\\. Web Research Tools**\n\nBoth agent patterns include:\n\n\\- \\`browse_web()\\`: Tavily-powered web search with configurable result counts\n\n\\- \\`get_website_map()\\`: Site structure analysis for systematic exploration\n\n\\- \\`get_website_content()\\`: Targeted content extraction from specific URLs\n\n  \n\n**Evaluation System**\n\n  \n\n**1 . Human-Verified Test Dataset (\\`src/evals/human_verified_searches.py\\`)**\n\n\\- **716 lines** of meticulously curated test cases\n\n\\- Two complexity levels: narrow-scope and broad-scope searches\n\n\\- Real-world scenarios: University of Alberta faculty searches\n\n\\- Ground truth data with complete contact information\n\n  \n\n**2\\. Automated Evaluation** (\\`eval_runner.py\\`):\n\n\\- DeepEval integration with custom GEval metrics\n\n\\- LLM-based correctness scoring\n\n\\- Recall and precision tracking\n\n\\- Visual lead comparison before evaluation\n\n  \n\n**3\\. Rich Visual Comparison** (\\`utils/lead_comparison.py\\`):\n\n\\- Field-by-field lead comparison with color coding\n\n\\- Match detection based on name/email similarity\n\n\\- Missing/extra lead identification\n\n\\- Comprehensive summary statistics with recall calculations\n\n  \n\n**Results & Performance Analysis**\n\n  \n\nSystematic evaluation across multiple LLM models:\n\n\\- **OpenAI**: GPT-4.1, GPT-4.1-mini, GPT-4.1-nano\n\n\\- **Anthropic**: Claude Sonnet 4, Claude 3.7 Sonnet, Claude 3.5 Sonnet\n\n**Evaluation Metrics**\n\n\\- **G-Eval Score**: LLM-based correctness assessment\n\n\\- **Recall Rate**: Percentage of expected leads successfully found\n\n  \n\n**Key Findings**\n\n  \n\n\\- **Limitations of the analysis**: only one run was done for each of the model combinatinations. There is a good amount of variance per run so a proper test should have multiple runs per grouping (I haven't done that due to time and $$ for now).\n\nFor more context check the images shared!\n\n  \n\n**✅ Interesting findings**\n\n\\- Stronger models as orchestrators are quite essential for good use of the sub-agents. Stronger models for the sub-agents might not be as essential and/or have not as strong compartive gains (very sensible due to limitations on my dataset).\n\n\\- Stronger models are more aggressive in terms of tool calls\n\n\\- Parallel tool calling seems to be something used by a wide range of models and it reallys speeds up the inference time\n\n  \n\n**🚧 Identified Limitations**\n\n\\- Models allucinate a lot on contact information on the lack more details (example: there is a mention of the name on a article but not contact info is shared. The model will then come up with fake info for the contact once it knows the person exists)\n\n\\- The search paths are very narrow and and can totally get out of correct path when faced with some timeout or other types of page retrieval errors.\n\n\\- There is an important need to have a fact check phase. The model tends to ignore important filtering cases like WHERE and CONTEXT clausese. This normally happens when models expand search and then the filters get \"lost\" in their context or attention\n\n  \n\n**Planned Improvements**\n\n\\- **Enhanced Fact-Checking**: Stronger validation pipeline to prevent filter drift after first submission of leads\n\n\\- **Broader Search Exploration**: Multiple parallel research paths with fallback strategies\n\n\\- **Semantic Lead Comparison**: Embedding-based similarity instead of string matching\n\n\\- **Real-time Lead Scoring**: Dynamic quality assessment and prioritization\n\n\\- **Individual Field Evaluation**: Granular accuracy metrics for each contact field",
      "github_urls": [
        "repo](https://github.com/fmeiraf/deep-leads",
        "https://github.com/fmeiraf/deep-leads"
      ],
      "categories": [
        "Finance/Trading",
        "Infrastructure/Tooling",
        "Classification/Analysis",
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Web Application",
        "Research/Experimentation",
        "Multi-Agent Orchestration"
      ],
      "technologies": [
        "PydanticAI",
        "OpenAI",
        "Anthropic"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 5681,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": true,
        "mentions_results": true,
        "mentions_future_work": false,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 1.0,
        "completeness_score": 1.0,
        "overall_quality": 1.0
      },
      "created_at": "2025-06-24T03:53:45.005985Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 6
    },
    {
      "user_name": "Brendan Rappazzo",
      "user_id": "cohort_20383_user_646813",
      "project_text": "I visited home this past weekend and was surprised to see just how much my mom is using ChatGPT now - she went from barely using it just ~5 months ago, to now it has almost completely replaced Google for her. And in ways I didn't expect - like not just general knowledge queries - but also things like \"What's the best place to get furniture near York, PA\" etc.  \n  \nIt got me thinking (as I think a lot of more business minded people are thinking) - what/how do LLMs recommend results from queries?  \n  \nInspired by this I build an agent that takes a query like \"What is the best park in Manhattan?\" - and is given the tools to call many other LLMs (for now just other OpenAI models - but of course this could be expanded. And is tasked with figuring out how and what to prompt these other LLMs to try and get an understanding of what their recommendations are - what do they agree on, what do they disagree on, are there any interesting suggestions, how do the suggestions differ based off the prompted question etc.  \n  \nPretty simple for now - but I am interested in building on this a lot actually - adding all major LLMs - encouraging the model to really query in multiple ways, and generating a nice report. Even adding some LLM as a judge to give feedback on the report and have it be regenerated if it's not good enough.  \n  \nHere is the Github link: [https://github.com/brendanhogan/simple_consensus_agent/tree/main](https://github.com/brendanhogan/simple_consensus_agent/tree/main)",
      "github_urls": [
        "https://github.com/brendanhogan/simple_consensus_agent/tree/main](https://github.com/brendanhogan/simple_consensus_agent/tree/main",
        "https://github.com/brendanhogan/simple_consensus_agent/tree/main",
        "https://github.com/brendanhogan/simple_consensus_agent"
      ],
      "categories": [
        "Web Application",
        "Education/Learning",
        "Conversational AI",
        "Single Agent Application"
      ],
      "technologies": [
        "OpenAI"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 3,
        "description_length": 1492,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": false,
        "mentions_challenges": false,
        "mentions_results": false,
        "mentions_future_work": false,
        "code_references": true,
        "structured_presentation": false,
        "documentation_score": 0.2,
        "completeness_score": 0.75,
        "overall_quality": 0.475
      },
      "created_at": "2025-06-24T03:57:59.031891Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 2
    },
    {
      "user_name": "Nick DeLuca",
      "user_id": "cohort_20383_user_648520",
      "project_text": "I wrote an LLM agent that attempts to one-shot vimgolf solutions at [https://github.com/nddeluca/prod-ready-agents-project/blob/main/vimgolf_solver.py](https://github.com/nddeluca/prod-ready-agents-project/blob/main/vimgolf_solver.py) to test how well models can edit text using vim normal and insert modes.  \n  \nI initially ventured into creating a tool with multiple turns, but ended up with a prompt and output for simplicity. Future versions could integrate as a tool call for multiple round trial and error.  \n\nThe most challenging part was setting up the eval environment. The examples, I used vimgolf.com to select 10 entries. The prompt gives the initial text and the target text, and prompts the model to return the sequence of vim commands that transform the initial text into the target text.  \n  \nThe solution is then evaluated by running a headless nvim instance with the initial text in the buffer, then sending the commands generated by the LLM. The LLM scores a 1 if the output matches, and a zero otherwise. This could be extending to reward fewer commands, but it seems difficult to normalize across examples without knowing the best solution. In addition, the task seems difficult for LLM's to do correctly on the first try, or the evaluation has issues. I see a 30% pass rate with the examples chosen for gpt-4.1 and only the same for o4-mini. In addition, the results do not seem highly deterministic and can vary between runs and models. However, more work still needs to be done to fully validate the evals are correctly working for each example (I'm not confident the vim simulation works 100% of the time or for all sequences), and more test cases need to be added for a better sample size.\n\nI chose this example because the reward function can be extended, and there is a large amount of freedom and complexity in the sequences that should work well in a RL environment.",
      "github_urls": [
        "https://github.com/nddeluca/prod-ready-agents-project/blob/main/vimgolf_solver.py",
        "https://github.com/nddeluca/prod-ready-agents-project/blob/main/vimgolf_solver.py](https://github.com/nddeluca/prod-ready-agents-project/blob/main/vimgolf_solver.py"
      ],
      "categories": [
        "Evaluation/Benchmarking",
        "Single Agent Application",
        "Reinforcement Learning"
      ],
      "technologies": [
        "OpenAI"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 1896,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": false,
        "mentions_challenges": false,
        "mentions_results": false,
        "mentions_future_work": true,
        "code_references": true,
        "structured_presentation": false,
        "documentation_score": 0.2,
        "completeness_score": 0.75,
        "overall_quality": 0.475
      },
      "created_at": "2025-06-24T04:02:32.028461Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 3
    },
    {
      "user_name": "Trivan Menezes",
      "user_id": "cohort_20383_user_403703",
      "project_text": "My agent is aimed at performing **sub-query generation for an agentic RAG pipeline**.\n\nNotebook repo: https://github.com/ttmenezes/agentic-rag-tuning\n\nI measure the quality of sub-queries generated by a model by testing the recall against a synthetic test set of questions mapped to chunk IDs across a few vector search indexes, and I also included text formatting reward functions.\n\nI tested the three models in the GPT 4.1 series, as well as DeepseekV3.\n\n_Results_\n\nMost of the four models scored well, but sometimes GPT 4.1 nano and GPT 4.1 missed out on reward points because the XML they generated was missing the closing tag </ answer >, so the formatting reward returned a low score, and the actual vector search could not run.\n\nMy goal is to use GRPO with my current reward functions to fine-tune a really small model to perform very well on this task!",
      "github_urls": [
        "https://github.com/ttmenezes/agentic-rag-tuning"
      ],
      "categories": [
        "Evaluation/Benchmarking",
        "Single Agent Application",
        "Reinforcement Learning"
      ],
      "technologies": [],
      "quality_metrics": {
        "has_github": true,
        "github_count": 1,
        "description_length": 860,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": false,
        "mentions_challenges": false,
        "mentions_results": false,
        "mentions_future_work": false,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 0.4,
        "completeness_score": 0.75,
        "overall_quality": 0.575
      },
      "created_at": "2025-06-24T05:30:39.215752Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 3
    },
    {
      "user_name": "Ari Pritchard-Bell",
      "user_id": "cohort_20383_user_647604",
      "project_text": "I want to leverage multimodal LLMs to automate plot digitization. This is still often done manually because of the high degree of ambiguity and reasoning required. However, the naive approach is a zero shot version where the LLM is asked to provide the points from a plot. This is similar to what was done here: [https://arxiv.org/html/2503.12326v1](https://arxiv.org/html/2503.12326v1). Unfortunately, zero shot is prone to the same hallucination issues that come with not using REPL to verify code output/analytical calculations.\n\nMy goal was to test zero shot on synthetic data with different models and attempt to improve the performance with tools.\n\nI'm attaching my zero shot results with two different 3.5 sonnet (v1 vs v2) models (git repo coming soon). Performance degrades drastically with increased data plotted. My dream is to use a local LLM with Ollama and I tested the qwen2.5VL with very poor results. It is my goal to get this working well by supplementing it with tools.\n\nI made a few attempts building tooling to help with this task. I believe this will improve results. I tried overlaying a grid and found that overlaying with lettered grid values made it easier for the LLM to distinguish from the underlying plot. I was unable to construct the tool use within an agentic pattern that improved performance...yet.\n\nFeedback:\n\nI would love to hear any ideas about giving an agent more \"standard\" computer vision tools, or a grid like I'm working on. Also, strategies on how to configure the multimodal components within an agentic framework. This is tricky, although there is a ton of overlap with the fascinating topic of computer use (see [https://github.com/OthersideAI/self-operating-computer](https://github.com/OthersideAI/self-operating-computer), [https://www.anthropic.com/news/3-5-models-and-computer-use](https://www.anthropic.com/news/3-5-models-and-computer-use)).",
      "github_urls": [
        "https://github.com/OthersideAI/self-operating-computer](https://github.com/OthersideAI/self-operating-computer)",
        "https://github.com/OthersideAI/self-operating-computer"
      ],
      "categories": [
        "Web Application",
        "Single Agent Application",
        "Infrastructure/Tooling"
      ],
      "technologies": [
        "Anthropic"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 1896,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": false,
        "mentions_results": true,
        "mentions_future_work": true,
        "code_references": true,
        "structured_presentation": false,
        "documentation_score": 0.6,
        "completeness_score": 1.0,
        "overall_quality": 0.8
      },
      "created_at": "2025-06-24T05:33:10.790818Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 8
    },
    {
      "user_name": "James E Snewin",
      "user_id": "cohort_20383_user_654542",
      "project_text": "I have a created a git-agent that can do simple git operations (like creating a local repository) within a containerised environment. This implementation is purposely lean (lack of SDKs and abstractions) for the sake of learning and understanding the core agent loop, and documents the limitations of the repo that could be future improvements.\n\n[https://github.com/tsnewnami/git-agent](https://github.com/tsnewnami/git-agent)\n\nSome things I struggled with are and would love some feedback on:\n\n\\- Having the agent capture a strong signal for when the goal was complete. I tried using answer tags, but had to check for the existence of keywords like \"complete\" or \"finished\" in the thinking tags. I tried doing prompt engineering and using pydantic, but could not get it to get use the answer tags as a completion signal.\n\n\\- I also noticed the agent redundantly executing commands, like multiple \\`ls\\` calls to validate the existence of the repo, even though it already validated this in the previous step and the result was appended to the conversation. (I noticed that anthropics multi-agent blog post faced a similar issue with their search sub-agents, doing redundant work).\n\nThank you,\n\nJames.",
      "github_urls": [
        "https://github.com/tsnewnami/git-agent](https://github.com/tsnewnami/git-agent",
        "https://github.com/tsnewnami/git-agent"
      ],
      "categories": [
        "Education/Learning",
        "Conversational AI",
        "Document Processing",
        "Multi-Agent Orchestration"
      ],
      "technologies": [
        "Anthropic"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 1200,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": true,
        "mentions_results": true,
        "mentions_future_work": true,
        "code_references": true,
        "structured_presentation": false,
        "documentation_score": 0.8,
        "completeness_score": 1.0,
        "overall_quality": 0.9
      },
      "created_at": "2025-06-24T08:42:13.242241Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {
        "clap": 1
      },
      "attachments_count": 2
    },
    {
      "user_name": "Hamza",
      "user_id": "cohort_20383_user_664608",
      "project_text": "We work at a cybersecurity company and wanted to make an agent to query logs from an [EDR](https://en.wikipedia.org/wiki/Endpoint_detection_and_response) to answer user questions (e.g: what happened on a client device in terms of process or file events)\n\nThe tool we chose was SentinelOne because we have a test instance available to query logs.\n\nOur agent will be able to perform queries to the Deep Visibility endpoint of SentinelOne using a tool call. Unfortunately there is no publicly available documentations for this endpoint (only UI based examples on the internal platform).\n\nTo teach the model the syntax, we tried few shot prompting using internal queries performed by our company.\n\nIn this [notebook](https://colab.research.google.com/drive/18Th0QvWKoq_VLpt4E9erP572cYB4wO54), you will find the different steps we took to build the agent using OpenAI agent SDK:\n\n*   Few shot prompting\n*   Output validation\n*   Tool calling\n*   Basic evaluation\n\nThe notebook does not contain all the utils/data used to make the agent work, as some of those are private.\n\nAs noted at the end of the notebook, the next steps are to change the framework and try it with smaller O/S models.\n\nWe are looking forward to learn more about RLVR and especially GRPO to improve this agent especially for more complex queries where it works sometimes but not consistently.\n\nThanks,\n\nHugo Rosenkranz & Hamza Sayah - Qevlar AI",
      "github_urls": [],
      "categories": [
        "Document Processing",
        "Infrastructure/Tooling",
        "API/Backend Service",
        "Evaluation/Benchmarking",
        "Web Application",
        "Research/Experimentation",
        "Single Agent Application"
      ],
      "technologies": [
        "OpenAI"
      ],
      "quality_metrics": {
        "has_github": false,
        "github_count": 0,
        "description_length": 1409,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": false,
        "mentions_results": false,
        "mentions_future_work": true,
        "code_references": false,
        "structured_presentation": false,
        "documentation_score": 0.4,
        "completeness_score": 0.5,
        "overall_quality": 0.45
      },
      "created_at": "2025-06-24T10:23:54.285876Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 2
    },
    {
      "user_name": "Ilkka Lehto",
      "user_id": "cohort_20383_user_647129",
      "project_text": "My goal is to build a model provider agnostic (multi-)agent chat interface where the user can perform data analysis based on input files, other input data sources like document based RAG using a code_interpreter tool to generate and execute python scripts and finally present the results acquired together with the agent in a powerpoint presentation.  \n  \nI started with an OpenAI Assistants API based solution where the OpenAI internal code_interpreter solution is ready to be used. I built a python-pptx based tool to create powerpoint presentations using a fixed master slide and the content from the LLM. The OpenAI based solution is also a baseline for all further improvements.  \n  \nNext I started to look for a solution towards the model provider agnostic goal and looked into PydanticAI as a solution to standardizing the LLM communication. First I implemented a Wrapper for the Tools using PydanticAI and Pydantic classes defining the inputs and autogeneration in serializing the schema from the docstring. There I already ran into the problem of it not being Assistants API format compatible. I extended the PydanticAI Tool class to a OpenAIAssistantTool class with the serialization of the schema done in the Assistants API compatible way. The next problem I ran into was the fact that the Assistants API based model was also not supported in the models out of the box. Also I was having some trouble getting the Bedrock Converse API based models to work in our enterprise setting and was worried to have to change too much of what I'd built based on the Assistants API towards my GUI being constrained to the opinionated frameworks so I decided park the frameworks for now and build my own Converse and OpenAI Chat Completion Agent classes where I wrap the LLM calls and handle the returned streaming responses.  \n\nExpanding the capability related to the python-pptx based powerpoint tool has been limited on the one hand due to bugs in the python-pptx library and on the other hand due to the lack of vision capability in the Azure OpenAI Assistants API GPT-4o. In the future it might be interesting to add a tool that takes care of the formatting of the slides based on a vision LLM?\n\nFor the internal code_interpreter tool I used a LangGraph based multi-agent solution where the file format is analyzed, then code is generated and afterwards executed and finally the results are evaluated. Based on the evaluation we either iterate further up until a fixed iteration limit or return the tool call results to the main orchestrator LLM. Here I've been tweaking on the multi-turn behaviour of the \"tool\" injecting chat history from the main orchestrator into the tool call to give the code generator better context on what has happened in the previous iterations. I've also given the main orchestrator the ability to input its suggestion on how the code should be like when it triggers the code_interpreter. On one hand this has lead to improved results in a longer multi-turn discussion with the user, on the other hand it sometimes leads to unnecessary iterations being taken either inside the code_interpreter tool or from the direction of the orchestrator LLM. In other words they keep iterating on a result that is already \"good enough\".\n\nAs a part of the topic multi-turn interactions I have implemented my own state management for the agents storing the chat history into a database and deserializing from there to invoke the LLM based on the chat history. It is its own bundle of topics/problems/solutions on how to give the LLM relevant information, how to handle the context size limits while growing the history in a chat session etc.  \n\nThe data analysis prompt answer pairs I want to evaluate consist of reading and understanding the input file format to find out the relevant input, extracting it from the data, some sorting, ranking, comparing (movement analysis) and formatting of the found data as well as generating visualizations. Even though I might be able to constrain the numerical outputs of my prompts into a format that could be checked deterministically I choose to use LLM as a judge with ground truth answers given by me. On evaluating the visualizations (plots, diagrams, powerpoints) I am unsure how to go about them. Perhaps a vision capable LLM could work as a judge there as well?  \n  \nThe queries I have put the Azure OpenAI Assistants API based GPT-4o model already to its limits. Switching to using Claude 3.7 I noticed a significant jump in performance in coming to the right solution reliably. On the other hand the Assistants API based solution worked a lot faster than the Claude 3.7 based internal solution where one sees that Claude 3.7 really likes to take its time and sometimes also likes to exaggerate on the amount of analysis it performs. Sometimes one feels one gets more than one is asking for.  \n\nA general observation I've had is that to get real useful outputs from the agent one needs to feed it as much (internal) data as possible. From that point of view I am very interested in extending the data sources available for the agent through MCPs. I am also very interested in ways to improve the document search capabilities away from the traditional RAG embeddings towards a more agentic ways of doing it.\n\nAnother general thought is that with the increasing capabilities of the reasoning models one is inclined to think in the direction of asynchronous long running tasks, letting the agentic system to iterate as long as it sees necessary to perform the analysis tasks. This obviously brakes a bit the traditional chat interface where one likes to get answers in a synchronous way.\n\nIn all this one more aspect is how the parts of the system could be trained further to do an even more effective job?",
      "github_urls": [],
      "categories": [
        "Finance/Trading",
        "Document Processing",
        "Infrastructure/Tooling",
        "API/Backend Service",
        "Classification/Analysis",
        "Evaluation/Benchmarking",
        "Web Application",
        "Conversational AI",
        "Multi-Agent Orchestration"
      ],
      "technologies": [
        "Azure",
        "Python",
        "OpenAI",
        "Anthropic",
        "PydanticAI"
      ],
      "quality_metrics": {
        "has_github": false,
        "github_count": 0,
        "description_length": 5769,
        "has_attachments": false,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": true,
        "mentions_results": true,
        "mentions_future_work": true,
        "code_references": true,
        "structured_presentation": false,
        "documentation_score": 0.8,
        "completeness_score": 0.75,
        "overall_quality": 0.775
      },
      "created_at": "2025-06-24T12:43:09.288056Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 0
    },
    {
      "user_name": "Julian - Thomas Erdoedy",
      "user_id": "cohort_20383_user_664556",
      "project_text": "\\# **Activity Recommendation Agent**\n\nMulti-turn conversational agent for personalized activity recommendations with enhanced web scraping and detailed extraction.\n\n  \n\n  \n\n\\## **✨ Features**\n\n\\- **Global Activity Search**: Find activities anywhere in the world\n\n\\- **Real-time Weather Integration**: Uses OpenWeather API for context -> so it does not recommend an outdoor run during a thunderstorm :)\n\n\\- **Adaptive Web Scraping**: Iterative page analysis that prevents data mix-up\n\n\\- **Accurate Data Extraction**: Individual activity pages vs. list page detection\n\n\\- **Type-safe Agent Pipeline**: Built with OpenAI Agents SDK and Pydantic models\n\n\\- **Interactive Chat Interface**: Terminal chat experience\n\n\\- **Intelligent Feedback Processing**: LLM-powered feedback analysis with context preservation\n\n\\- **Adaptive Search**: Smart preference evolution maintains context while incorporating refinements\n\n  \n\n  \n\n\\## **Assignment Mapping**\n\n**Multi-turn Interaction & Tool Use:**\n\n\\- Conversational agent with feedback loops (3-5 turns)\n\n\\- 4 specialized tools: weather API, web search, smart content scraping, feedback analysis\n\n\\- Advanced preference evolution: maintains context while incorporating user refinements\n\n**Agent Scaffold:**\n\n\\- Built using OpenAI Agents SDK with specialized agents:\n\n\\- \\`IntentAgent\\`: Parses requests + gets weather data with caching\n\n\\- \\`ExtractionAgent\\`: Smart web search + intelligent content scraping\n\n\\- \\`ConversationAgent\\`: Response generation + feedback analysis\n\n**Problem Identification & Adjustment:**\n\n\\- **Fixed**: Preference evolution (preserves activity type/location while updating preferences)\n\n\\- **Fixed**: Data mixing prevention (smart page analysis prevents mixing metrics from different activities)\n\n\\- **Enhanced**: Detailed metrics extraction with strict no-hallucination policy\n\n**Test Prompts:** Comprehensive test set created and validated for accuracy and relevance\n\n**Multi-model Testing:** Limited testing with alternative models (nano) showed inconsistent results - missing location restrictions (suggesting 2000km routes when asking for local Vienna activities) and obvious outputs (bike equipment: \"a bicycle\"). Reliable results currently only with GPT-4o-mini.\n\n  \n\n\\### **❌ Not Yet Implemented**\n\n**Reward Function:** No formal evaluation metrics implemented\n\n**Best-of-N Selection:** Not implemented\n\n**Parallel/Multi-agent:** Single-threaded implementation\n\nExpect for the test prompts I did not have time to think about a proper reward function. Schema validation is mainly achieved by instructor, so I think a \"plausibility\" best-of-n reward function would be ideal. this could be done at scale through user feedback, or through an LLM judge. both of which I want to implement further down the road.\n\n  \n\n  \n\n\\## **Architecture (also see image)**\n\n**Enhanced Pipeline:**\n\n1\\. **Intent Parsing**: Extract structured parameters from user query + weather with caching\n\n2\\. **Adaptive Web Search**: Geographic-specific queries with enhancement\n\n3\\. **Intelligent Page Analysis**: Classify page type and determine extraction strategy\n\n4\\. **Targeted Content Extraction**:\n\n\\- Individual pages: Direct detailed extraction\n\n\\- List pages: Sub-URL following or candidate selection\n\n\\- Mixed content: Conservative extraction\n\n5\\. **Response Generation**: Rich display with verified metrics\n\n6\\. **LLM Feedback Analysis**: Context-preserving preference evolution\n\n**Context-Preserving Feedback Evolution:**\n\n\\`\\`\\`\n\nTurn 1: \"running route in Vienna\"\n\n→ Gets: General Vienna running routes\n\nTurn 2: \"longer routes, more in outskirts\"\n\n→ Classification: REFINEMENT (not new search)\n\n→ Preserves: running, Vienna\n\n→ Updates: duration (longer), location (outskirts)\n\n→ New search: \"long running routes Vienna outskirts\"\n\nTurn 3: \"actually prefer trails over pavement\"\n\n→ Preserves: running, Vienna, outskirts, longer duration\n\n→ Updates: surface preference (trails)\n\n→ Result: \"long trail running routes Vienna outskirts\"\n\n\\`\\`\\`\n\n  \n\n  \n\n\\## **Current Challenges & Limitations**\n\n**Preference Conflict Resolution:** User: \"closer\" then later \"distance doesn't matter but want different terrain\"\n\n**Recommendation Diversity:** Search engines may return similar results repeatedly\n\n**Success Metrics Definition:** Conversation success measurement unclear\n\n**Scraping Scope Limitations:**\n\n\\- Some sites block or limit scraping\n\n\\- Dynamic content may not be captured\n\n\\- Tuned for activities or list of activities, scheduled activities are still weak due to lack to context (mostly time)\n\n  \n\n  \n\n\\## **Next Steps**\n\n**Evaluation Framework:**\n\n\\- Formal test prompt sets for different activity types and locations\n\n\\- Reward functions: preference satisfaction, detail completeness, conversation flow\n\n\\- A/B testing framework for different strategies\n\n\\- Best-of-N selection with evaluation functions\n\n**Handoffs and Guardrails: OpenAI Agents SDK**\n\n\\- Multi-agent collaboration for complex queries with handoffs\n\n\\- Explicitly restricting what the agent can answer (guardrails)\n\n**Enhanced Context & Scheduling:**\n\n\\- Time-of-day aware recommendations for scheduled activities (yoga sessions, boxing classes, gym hours)\n\n\\- Integration with local business hours and class schedules\n\n\\- Dynamic availability checking for time-sensitive activities\n\n\\- Smart scheduling optimization based on user's available time windows\n\n**Dynamic/Advanced Scraping:**\n\n\\- Deep course schedule extraction from fitness studios, gyms, and activity providers\n\n\\- Multi-level page navigation to find buried scheduling information\n\n\\- Structured data extraction from complex timetables and booking systems\n\n\\- Real-time availability parsing from dynamic content and booking platforms\n\n\\- Smart form interaction for accessing protected schedule information\n\n\\- Calendar integration and recurring event pattern recognition\n\n**Game Integration:**\n\n\\- Hook into my mobile gaming system where activities (running, cycling) generate XP based on player's remaining \"game-energy\"\n\n\\- Smart XP optimization queries: \"I have 2 hours and want to maximize my XP with the energy I have left, what are my options?\"\n\n\\- Activity difficulty/reward calculation based on player stats and energy levels\n\n\\- Real-world activity tracking integration with game progression\n\n  \n\n  \n\n\\## **Tech Stack**\n\n\\- **Framework**: OpenAI Agents SDK with async execution\n\n\\- **APIs**: OpenWeather (cached), Firecrawl (search + scraping)\n\n\\- **Models**: GPT-4o-mini (intent, extraction, conversation, analysis)\n\n\\- **Language**: Python + asyncio with comprehensive error handling\n\n\\- **Type Safety**: Pydantic + Instructor for structured outputs\n\n\\- **Architecture**: Agent-based pipeline with decent orchestration\n\n[https://github.com/j4y33/activity_recommender](https://github.com/j4y33/activity_recommender)",
      "github_urls": [
        "https://github.com/j4y33/activity_recommender](https://github.com/j4y33/activity_recommender",
        "https://github.com/j4y33/activity_recommender"
      ],
      "categories": [
        "Finance/Trading",
        "Infrastructure/Tooling",
        "Classification/Analysis",
        "Reinforcement Learning",
        "API/Backend Service",
        "Web Application",
        "Evaluation/Benchmarking",
        "Data Processing/ETL",
        "Multi-Agent Delegation",
        "Education/Learning",
        "Conversational AI",
        "Multi-Agent Orchestration"
      ],
      "technologies": [
        "OpenAI",
        "Python"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 6821,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": true,
        "mentions_results": true,
        "mentions_future_work": true,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 1.0,
        "completeness_score": 1.0,
        "overall_quality": 1.0
      },
      "created_at": "2025-06-24T12:46:43.021419Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 3
    },
    {
      "user_name": "Kazuki Inamura",
      "user_id": "cohort_20383_user_378036",
      "project_text": "The detail report in the repo is here: [https://github.com/kzinmr/nutrition-agent/blob/main/EXPERIMENT_REPORT.md](https://github.com/kzinmr/nutrition-agent/blob/main/EXPERIMENT_REPORT.md)\n\n\\# Seeking RL Expertise for Nutrition Planning Agent\n\nI've been developing an AI-powered meal planning agent that creates balanced 3-day meal plans from available ingredients. While the system works reasonably well (96% constraint satisfaction), I'm hitting fundamental limitations that I believe RL could solve.\n\n\\## Current Architecture & Performance\n\n\\- **Single-agent LLM** with tool-calling to FatSecret API for nutrition validation\n\n\\- **Best model**: GPT-4.1-mini achieving 76.5% overall score at $0.03/run\n\n\\- **Key weakness**: Low nutrition target accuracy (struggling with precise PFC ratios)\n\n\\## Core Challenges\n\n**1\\. Sequential Decision Making**: The agent must plan 9 interdependent meals (3 days × 3 meals) where early choices constrain later options, and currently using flat state representation.\n\n**2\\. Exploration-Exploitation**: Agent converges to \"safe\" repetitive meals (rice + chicken). Need strategies to maintain diversity while meeting strict nutritional targets.\n\n**3\\. Hard Constraints**: Zero tolerance for allergen violations.\n\n**4\\. Credit Assignment**: When a 3-day plan fails nutritionally, it's unclear which specific meals caused the failure.\n\n**5\\. Reward Design**: Linear penalties lead to consistent 20% nutrition errors rather than attempting exact matches.\n\n\\## Specific Questions\n\n\\- For this combinatorial optimization with safety constraints, what kind of RL approach be more appropriate?\n\n\\- How would you handle the ingredient inventory depletion across sequential meal decisions?\n\n\\- Is there value in multi-agent RL here (nutrition agent, inventory agent, quality agent)?\n\nThe current reactive architecture lacks strategic planning. I believe RL could enable proactive optimization while maintaining our successful tool-based validation. Any insights on state representation, reward shaping, or exploration strategies would be invaluable.",
      "github_urls": [
        "https://github.com/kzinmr/nutrition-agent/blob/main/EXPERIMENT_REPORT.md](https://github.com/kzinmr/nutrition-agent/blob/main/EXPERIMENT_REPORT.md",
        "https://github.com/kzinmr/nutrition-agent/blob/main/EXPERIMENT_REPORT.md"
      ],
      "categories": [
        "Research/Experimentation",
        "API/Backend Service",
        "Multi-Agent Orchestration"
      ],
      "technologies": [
        "React",
        "OpenAI"
      ],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 2075,
        "has_attachments": true,
        "detailed_description": true,
        "mentions_technical_details": true,
        "mentions_challenges": false,
        "mentions_results": true,
        "mentions_future_work": true,
        "code_references": true,
        "structured_presentation": true,
        "documentation_score": 0.8,
        "completeness_score": 1.0,
        "overall_quality": 0.9
      },
      "created_at": "2025-06-24T15:02:22.517109Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 2
    },
    {
      "user_name": "Harry Jackson",
      "user_id": "cohort_20383_user_665546",
      "project_text": "My experiment was to make an agent to solve cryptic crosswords. It was not totally successful! But I think the approach has potential.\n\n[https://github.com/harry-jackson/agents_course/blob/main/agents_course_homework_week_1.ipynb](https://github.com/harry-jackson/agents_course/blob/main/agents_course_homework_week_1.ipynb)",
      "github_urls": [
        "https://github.com/harry-jackson/agents_course/blob/main/agents_course_homework_week_1.ipynb](https://github.com/harry-jackson/agents_course/blob/main/agents_course_homework_week_1.ipynb",
        "https://github.com/harry-jackson/agents_course/blob/main/agents_course_homework_week_1.ipynb"
      ],
      "categories": [
        "Research/Experimentation",
        "Education/Learning",
        "Single Agent Application"
      ],
      "technologies": [],
      "quality_metrics": {
        "has_github": true,
        "github_count": 2,
        "description_length": 324,
        "has_attachments": true,
        "detailed_description": false,
        "mentions_technical_details": false,
        "mentions_challenges": false,
        "mentions_results": false,
        "mentions_future_work": false,
        "code_references": true,
        "structured_presentation": false,
        "documentation_score": 0.0,
        "completeness_score": 0.5,
        "overall_quality": 0.25
      },
      "created_at": "2025-06-24T16:33:47.046478Z",
      "channel_name": "Prototype an Agent",
      "reaction_counts": {},
      "attachments_count": 2
    }
  ]
}